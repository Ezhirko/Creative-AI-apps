{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2874,"status":"ok","timestamp":1737649850185,"user":{"displayName":"Ezhirko Arulmozhi","userId":"16774972310128518178"},"user_tz":-330},"id":"s9cpZmyTB1mY","outputId":"4285ff2c-d1f7-4c00-9ba1-04e7c846f8c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1737649850186,"user":{"displayName":"Ezhirko Arulmozhi","userId":"16774972310128518178"},"user_tz":-330},"id":"PMD6tyE7B_CD"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/ERAV3/Session13')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23678,"status":"ok","timestamp":1737649873853,"user":{"displayName":"Ezhirko Arulmozhi","userId":"16774972310128518178"},"user_tz":-330},"id":"qGYxBVB79uD9","outputId":"a718f7d4-b98e-4565-b225-3d4fa9076ec0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (1.26.4)\n","Requirement already satisfied: pyarrow\u003e=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (17.0.0)\n","Requirement already satisfied: dill\u003c0.3.7,\u003e=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (0.3.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (2.2.2)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (2.32.3)\n","Requirement already satisfied: tqdm\u003e=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (0.70.14)\n","Requirement already satisfied: fsspec\u003e=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]\u003e=2021.11.1-\u003edatasets==2.12.0) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (3.11.11)\n","Requirement already satisfied: huggingface-hub\u003c1.0.0,\u003e=0.11.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (0.27.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (24.2)\n","Requirement already satisfied: responses\u003c0.19 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (0.18.0)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets==2.12.0) (2.4.4)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets==2.12.0) (1.3.2)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets==2.12.0) (24.3.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets==2.12.0) (1.5.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets==2.12.0) (6.1.0)\n","Requirement already satisfied: propcache\u003e=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets==2.12.0) (0.2.1)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets==2.12.0) (1.18.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.11.0-\u003edatasets==2.12.0) (3.16.1)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.11.0-\u003edatasets==2.12.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.19.0-\u003edatasets==2.12.0) (3.4.1)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.19.0-\u003edatasets==2.12.0) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.19.0-\u003edatasets==2.12.0) (2.3.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.19.0-\u003edatasets==2.12.0) (2024.12.14)\n","Requirement already satisfied: python-dateutil\u003e=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas-\u003edatasets==2.12.0) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas-\u003edatasets==2.12.0) (2024.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas-\u003edatasets==2.12.0) (2024.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil\u003e=2.8.2-\u003epandas-\u003edatasets==2.12.0) (1.17.0)\n","Collecting transformers==4.30.0\n","  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0) (3.16.1)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0) (0.27.1)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0) (1.26.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0) (24.2)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0) (2.32.3)\n","Collecting tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 (from transformers==4.30.0)\n","  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors\u003e=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0) (0.5.2)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.0) (4.67.1)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.14.1-\u003etransformers==4.30.0) (2024.10.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.14.1-\u003etransformers==4.30.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.11/dist-packages (from requests-\u003etransformers==4.30.0) (3.4.1)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.11/dist-packages (from requests-\u003etransformers==4.30.0) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests-\u003etransformers==4.30.0) (2.3.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests-\u003etransformers==4.30.0) (2024.12.14)\n","Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.0\n","    Uninstalling tokenizers-0.21.0:\n","      Successfully uninstalled tokenizers-0.21.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.47.1\n","    Uninstalling transformers-4.47.1:\n","      Successfully uninstalled transformers-4.47.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sentence-transformers 3.3.1 requires transformers\u003c5.0.0,\u003e=4.41.0, but you have transformers 4.30.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.30.0\n"]}],"source":["!pip install datasets==2.12.0\n","!pip install transformers==4.30.0"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4898,"status":"ok","timestamp":1737649878745,"user":{"displayName":"Ezhirko Arulmozhi","userId":"16774972310128518178"},"user_tz":-330},"id":"d6fWAClW8Gsh"},"outputs":[],"source":["import os\n","import re\n","import time\n","import glob\n","import torch\n","import sys\n","from torch.cuda.amp import autocast, GradScaler\n","from transformers import AutoTokenizer\n","from datasets import load_dataset\n","from smolLM2 import SmolLM2\n","from torch.optim.lr_scheduler import LambdaLR"]},{"cell_type":"markdown","metadata":{"id":"4IhjWC-E97Bo"},"source":["#### Define the functions"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1737649878746,"user":{"displayName":"Ezhirko Arulmozhi","userId":"16774972310128518178"},"user_tz":-330},"id":"KGf3V27Z8tJC"},"outputs":[],"source":["def get_device():\n","    if torch.cuda.is_available():\n","        return torch.device(\"cuda\"), \"cuda\"\n","    elif torch.backends.mps.is_available():\n","        return torch.device(\"mps\"), \"mps\"\n","    else:\n","        return torch.device(\"cpu\"), \"cpu\"\n","\n","def load_checkpoint(checkpoint_path):\n","    \"\"\"Safely load checkpoint with error handling\"\"\"\n","    try:\n","        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","        return checkpoint\n","    except (RuntimeError, EOFError, Exception) as e:\n","        print(f\"\\nWarning: Failed to load checkpoint at {checkpoint_path}\")\n","        print(f\"Error: {str(e)}\")\n","        print(\"Starting training from scratch...\\n\")\n","        return None\n","\n","def get_lr_scheduler(optimizer, config):\n","    warmup_steps = config[\"optimizer\"][\"learning_rate_scheduler\"][\"lr_warmup_steps\"]\n","    decay_start = config[\"optimizer\"][\"learning_rate_scheduler\"][\"lr_decay_starting_step\"]\n","    decay_steps = config[\"optimizer\"][\"learning_rate_scheduler\"][\"lr_decay_steps\"]\n","    base_lr = config[\"optimizer\"][\"learning_rate_scheduler\"][\"learning_rate\"]\n","    min_lr = config[\"optimizer\"][\"learning_rate_scheduler\"][\"min_decay_lr\"]\n","\n","    def lr_lambda(step):\n","        if step \u003c warmup_steps:\n","            return step / warmup_steps\n","        elif step \u003c decay_start:\n","            return 1.0\n","        else:\n","            decay_ratio = (step - decay_start) / decay_steps\n","            decay_ratio = min(1.0, decay_ratio)\n","            return 1.0 - (1.0 - min_lr / base_lr) * decay_ratio\n","\n","    return LambdaLR(optimizer, lr_lambda)\n","\n","def get_latest_checkpoint(checkpoint_dir):\n","    \"\"\"Find the latest checkpoint in the directory based on step number.\"\"\"\n","    checkpoints = glob.glob(os.path.join(checkpoint_dir, \"step_*.pt\"))\n","    if not checkpoints:\n","        return None\n","\n","    # Extract step numbers and find the latest\n","    steps = [int(ckpt.split('step_')[-1].replace('.pt', '')) for ckpt in checkpoints]\n","    latest_checkpoint = checkpoints[steps.index(max(steps))]\n","    return latest_checkpoint\n","\n","def save_final_model(model, save_path):\n","    \"\"\"Save the final model in .pt format\"\"\"\n","    torch.save(model.state_dict(), save_path)\n","    print(f\"Saved final model to: {save_path}\")\n","\n","def is_valid_loss(loss_value):\n","    \"\"\"Check if loss value is valid\"\"\"\n","    return loss_value is not None and not torch.isnan(loss_value) and not torch.isinf(loss_value)\n","\n","def train():\n","    # Get the best available device\n","    device, device_name = get_device()\n","    print(f\"Using device: {device_name}\")\n","\n","    # Define configuration\n","    config = {\n","        \"checkpoints\": {\n","            \"checkpoint_interval\": 500,\n","            \"checkpoints_path\": \"checkpoints\",\n","            \"checkpoints_path_is_shared_file_system\": False,\n","            \"resume_checkpoint_path\": None,\n","            \"save_final_state\": False,\n","            \"save_initial_state\": False,\n","        },\n","        \"general\": {\n","            \"benchmark_csv_path\": None,\n","            \"consumed_train_samples\": None,\n","            \"ignore_sanity_checks\": True,\n","            \"project\": \"smollm2\",\n","            \"run\": \"smollm2-135M\",\n","            \"seed\": 8,\n","            \"step\": None,\n","        },\n","        \"logging\": {\n","            \"iteration_step_info_interval\": 1,\n","            \"log_level\": \"info\",\n","            \"log_level_replica\": \"info\",\n","        },\n","        \"model\": {\n","            \"ddp_bucket_cap_mb\": 25,\n","            \"dtype\": \"bfloat16\",\n","            \"init_method\": {\n","                \"std\": 0.041666666666666664,\n","            },\n","            \"make_vocab_size_divisible_by\": 1,\n","            \"model_config\": {\n","                \"bos_token_id\": 0,\n","                \"eos_token_id\": 0,\n","                \"hidden_act\": \"silu\",\n","                \"hidden_size\": 576,\n","                \"initializer_range\": 0.041666666666666664,\n","                \"intermediate_size\": 1536,\n","                \"is_llama_config\": True,\n","                \"max_position_embeddings\": 2048,\n","                \"num_attention_heads\": 9,\n","                \"num_hidden_layers\": 30,\n","                \"num_key_value_heads\": 3,\n","                \"pad_token_id\": None,\n","                \"pretraining_tp\": 1,\n","                \"rms_norm_eps\": 1.0e-05,\n","                \"rope_interleaved\": False,\n","                \"rope_scaling\": None,\n","                \"rope_theta\": 10000.0,\n","                \"tie_word_embeddings\": True,\n","                \"use_cache\": True,\n","                \"vocab_size\": 49152,\n","            },\n","        },\n","        \"optimizer\": {\n","            \"accumulate_grad_in_fp32\": True,\n","            \"clip_grad\": 1.0,\n","            \"learning_rate_scheduler\": {\n","                \"learning_rate\": 0.003,\n","                \"lr_decay_starting_step\": 1600000,\n","                \"lr_decay_steps\": 400000,\n","                \"lr_decay_style\": \"linear\",\n","                \"lr_warmup_steps\": 2000,\n","                \"lr_warmup_style\": \"linear\",\n","                \"min_decay_lr\": 0,\n","            },\n","            \"optimizer_factory\": {\n","                \"adam_beta1\": 0.9,\n","                \"adam_beta2\": 0.95,\n","                \"adam_eps\": 1.0e-08,\n","                \"name\": \"adamW\",\n","                \"torch_adam_is_fused\": True,\n","            },\n","            \"weight_decay\": 0.01,\n","            \"zero_stage\": 0,\n","        },\n","        \"parallelism\": {\n","            \"dp\": 64,\n","            \"expert_parallel_size\": 1,\n","            \"pp\": 1,\n","            \"pp_engine\": \"1f1b\",\n","            \"recompute_layer\": False,\n","            \"tp\": 1,\n","            \"tp_linear_async_communication\": True,\n","            \"tp_mode\": \"REDUCE_SCATTER\",\n","            \"tp_recompute_allgather\": True,\n","        },\n","        \"tokenizer\": {\n","            \"tokenizer_max_length\": None,\n","            \"tokenizer_name_or_path\": \"HuggingFaceTB/cosmo2-tokenizer\",\n","            \"tokenizer_revision\": None,\n","        },\n","        \"tokens\": {\n","            \"batch_accumulation_per_replica\": 1,\n","            \"limit_test_batches\": 0,\n","            \"limit_val_batches\": 0,\n","            \"micro_batch_size\": 8,\n","            \"sequence_length\": 2048,\n","            \"train_steps\": 5000,\n","            \"val_check_interval\": 500,\n","        },\n","    }\n","\n","    # Create checkpoints directory if it doesn't exist\n","    os.makedirs(config['checkpoints']['checkpoints_path'], exist_ok=True)\n","\n","    # Load model configuration\n","    model_config = config[\"model\"][\"model_config\"]\n","\n","    # Set random seed\n","    torch.manual_seed(config[\"general\"][\"seed\"])\n","\n","    # Initialize model\n","    model = SmolLM2(model_config)\n","\n","    # Initialize mixed precision training\n","    use_amp = device_name != \"cpu\"\n","    dtype = torch.float32\n","    if use_amp:\n","        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n","            dtype = torch.bfloat16\n","            scaler = None\n","            print(\"Using native bfloat16 mixed precision (no gradient scaling)\")\n","        else:\n","            dtype = torch.float16\n","            scaler = GradScaler()\n","            print(\"Using float16 mixed precision with gradient scaling\")\n","    else:\n","        scaler = None\n","        print(\"Using full precision (float32)\")\n","\n","    # Move model to device and set dtype\n","    model = model.to(device)\n","    if dtype != torch.float32:\n","        model = model.to(dtype)\n","\n","    # Initialize tokenizer with padding token\n","    tokenizer = AutoTokenizer.from_pretrained(config[\"tokenizer\"][\"tokenizer_name_or_path\"])\n","\n","    # Set padding token to eos token if pad token is not set\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","        print(\"Set padding token to EOS token\")\n","\n","    # Update model config with pad token id\n","    model_config[\"pad_token_id\"] = tokenizer.pad_token_id\n","\n","    # Load dataset with streaming and specific config\n","    dataset = load_dataset(\n","        \"HuggingFaceTB/smollm-corpus\",\n","        \"cosmopedia-v2\",\n","        streaming=True\n","    )\n","    train_dataset = dataset[\"train\"]\n","    print(\"Loaded cosmopedia-v2 dataset in streaming mode\")\n","\n","    def tokenize_function(examples):\n","        # Process the batch of texts\n","        tokenized = tokenizer(\n","            examples[\"text\"],\n","            truncation=True,\n","            max_length=config[\"tokens\"][\"sequence_length\"],\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Stack the tensors properly\n","        return {\n","            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),  # Remove batch dimension\n","            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0)  # Remove batch dimension\n","        }\n","\n","    # Initialize step counter and total steps\n","    initial_total_steps = config[\"tokens\"][\"train_steps\"]  # 5000\n","    extended_steps = 50\n","    final_total_steps = initial_total_steps + extended_steps\n","    step = 1  # Initialize step to 1\n","\n","    # Try to find latest checkpoint\n","    checkpoint_files = glob.glob(os.path.join(config['checkpoints']['checkpoints_path'], 'step_*.pt'))\n","    latest_checkpoint = None\n","\n","    if checkpoint_files:\n","        # Sort checkpoints by step number\n","        checkpoint_files.sort(key=lambda x: int(re.search(r'step_(\\d+)', x).group(1)))\n","        latest_checkpoint = checkpoint_files[-1]\n","\n","        print(f\"\\nFound checkpoint: {latest_checkpoint}\")\n","        checkpoint = load_checkpoint(latest_checkpoint)\n","\n","        if checkpoint is not None:\n","            try:\n","                model.load_state_dict(checkpoint['model_state_dict'])\n","                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","                step = checkpoint['step']  # Use loaded step directly\n","                print(f\"Resumed from step {step}\")\n","            except Exception as e:\n","                print(f\"Warning: Failed to restore checkpoint state: {str(e)}\")\n","                print(\"Starting training from scratch at step 1...\")\n","                step = 1  # Reset to 1 if checkpoint loading fails\n","\n","    # Initialize dataset iterator\n","    train_iter = iter(train_dataset.map(\n","        tokenize_function,\n","        remove_columns=train_dataset.column_names,\n","        batched=True,\n","        batch_size=config[\"tokens\"][\"micro_batch_size\"]\n","    ))\n","\n","    # Skip batches if resuming from checkpoint\n","    if step \u003e 1:\n","        print(f\"Skipping {step * config['tokens']['batch_accumulation_per_replica']} batches to resume position...\")\n","        for _ in range(step * config[\"tokens\"][\"batch_accumulation_per_replica\"]):\n","            try:\n","                next(train_iter)\n","            except StopIteration:\n","                train_iter = iter(train_dataset.map(\n","                    tokenize_function,\n","                    remove_columns=train_dataset.column_names,\n","                    batched=True,\n","                    batch_size=config[\"tokens\"][\"micro_batch_size\"]\n","                ))\n","                next(train_iter)\n","        print(\"Done skipping batches\")\n","\n","    # Initialize optimizer\n","    use_fused = config[\"optimizer\"][\"optimizer_factory\"][\"torch_adam_is_fused\"]\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(),\n","        lr=config[\"optimizer\"][\"learning_rate_scheduler\"][\"learning_rate\"],\n","        betas=(\n","            config[\"optimizer\"][\"optimizer_factory\"][\"adam_beta1\"],\n","            config[\"optimizer\"][\"optimizer_factory\"][\"adam_beta2\"]\n","        ),\n","        eps=config[\"optimizer\"][\"optimizer_factory\"][\"adam_eps\"],\n","        weight_decay=config[\"optimizer\"][\"weight_decay\"],\n","        fused=use_fused\n","    )\n","\n","    # Initialize scheduler\n","    scheduler = get_lr_scheduler(optimizer, config)\n","\n","    # Training parameters\n","    batch_size = config[\"tokens\"][\"micro_batch_size\"]\n","    accum_steps = config[\"tokens\"][\"batch_accumulation_per_replica\"]\n","    save_steps = 1000\n","    eval_steps = 500\n","    grad_clip = 1.0\n","\n","    print(\"\\nStarting training...\")\n","    print(f\"Total steps: {final_total_steps}\")\n","    print(f\"Device: {device_name}\")\n","    print(f\"{'='*50}\\n\")\n","\n","    while step \u003c= final_total_steps:  # Changed \u003c to \u003c= to include final step\n","        step_start_time = time.time()\n","        accumulated_loss = 0\n","\n","        # Training step\n","        for accum_step in range(accum_steps):\n","            try:\n","                batch = next(train_iter)\n","            except StopIteration:\n","                train_iter = iter(train_dataset.map(\n","                    tokenize_function,\n","                    remove_columns=train_dataset.column_names,\n","                    batched=True,\n","                    batch_size=config[\"tokens\"][\"micro_batch_size\"]\n","                ))\n","                batch = next(train_iter)\n","\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","\n","            if input_ids.dim() == 1:\n","                input_ids = input_ids.unsqueeze(0)\n","                attention_mask = attention_mask.unsqueeze(0)\n","\n","            if use_amp:\n","                with autocast(dtype=dtype):\n","                    outputs = model(input_ids, attention_mask)\n","                    loss = torch.nn.functional.cross_entropy(\n","                        outputs.view(-1, outputs.size(-1)),\n","                        input_ids.view(-1)\n","                    )\n","                    loss = loss / accum_steps\n","\n","                if scaler is not None:\n","                    scaler.scale(loss).backward()\n","                else:\n","                    loss.backward()\n","            else:\n","                outputs = model(input_ids, attention_mask)\n","                loss = torch.nn.functional.cross_entropy(\n","                    outputs.view(-1, outputs.size(-1)),\n","                    input_ids.view(-1)\n","                )\n","                loss = loss / accum_steps\n","                loss.backward()\n","\n","            accumulated_loss += loss.item() * accum_steps\n","\n","        # Optimizer step\n","        if scaler is not None:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            scaler.step(optimizer)\n","            scaler.update()\n","        else:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        # Get current learning rate\n","        current_lr = scheduler.get_last_lr()[0]\n","\n","        # Calculate step time and tokens/sec\n","        step_time = time.time() - step_start_time\n","        step_time_ms = step_time * 1000\n","        tokens_per_second = (config[\"tokens\"][\"micro_batch_size\"] * config[\"tokens\"][\"sequence_length\"]) / step_time\n","\n","        # Print step info\n","        print(f\"Step {step}/{final_total_steps} | Loss: {accumulated_loss:.4f} | LR: {current_lr:.6f} | \"\n","                    f\"Total Step Time: {step_time_ms:.2f}ms | \"\n","                    f\"Tokens/sec: {tokens_per_second:.2f} (accumulated over {config['tokens']['micro_batch_size']} batches)\")\n","\n","        # Text generation at step 500, 1000, 1500, etc.\n","        if (step % eval_steps) == 0:\n","            print(f\"\\n{'='*50}\")\n","            print(f\"Generating text sample at step {step}\")\n","            print(f\"{'='*50}\")\n","\n","            model.eval()\n","            with torch.no_grad():\n","                prompt = \"Once upon a time\"\n","                input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","                generated = model.generate(\n","                    input_ids,\n","                    max_length=200,\n","                    min_length=50,\n","                    num_return_sequences=1,\n","                    pad_token_id=tokenizer.pad_token_id,\n","                    do_sample=True,\n","                    temperature=0.8,\n","                    top_k=50,\n","                    top_p=0.95\n","                )\n","                generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n","                print(f\"Prompt: {prompt}\")\n","                print(f\"Generated text:\\n{generated_text}\")\n","                print(f\"{'='*50}\\n\")\n","            model.train()\n","\n","        # Checkpointing at step 1000, 2000, 3000, etc.\n","        if (step % save_steps) == 0:\n","            print(f\"\\n{'='*50}\")\n","            print(f\"Saving checkpoint at step {step}\")\n","            print(f\"{'='*50}\")\n","\n","            checkpoint_path = f\"{config['checkpoints']['checkpoints_path']}/step_{step}.pt\"\n","            try:\n","                torch.save({\n","                    'step': step,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'scheduler_state_dict': scheduler.state_dict(),\n","                    'loss': accumulated_loss,\n","                    'config': config,\n","                }, checkpoint_path)\n","                print(f\"Checkpoint saved to: {checkpoint_path}\")\n","                print(f\"{'='*50}\\n\")\n","\n","            except Exception as e:\n","                print(f\"Warning: Failed to save checkpoint: {str(e)}\")\n","                print(f\"{'='*50}\\n\")\n","\n","        # Increment step counter at the end\n","        step += 1\n","\n","    print(\"\\nTraining completed!\")\n","\n","    # Save final checkpoint and model\n","    final_checkpoint_path = f\"{config['checkpoints']['checkpoints_path']}/step_final_{final_total_steps}.pt\"\n","    torch.save({\n","        'step': step,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'loss': accumulated_loss,\n","        'config': config,\n","    }, final_checkpoint_path)\n","    print(f\"Saved final checkpoint: {final_checkpoint_path}\")\n","\n","    # Save final model in .pt format\n","    model_save_path = \"smollm2_final.pt\"\n","    torch.save(model.state_dict(), model_save_path)\n","    print(f\"Saved final model to: {model_save_path}\")\n","\n","    print(\"\\nTraining completed successfully!\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":8382,"status":"error","timestamp":1737649887121,"user":{"displayName":"Ezhirko Arulmozhi","userId":"16774972310128518178"},"user_tz":-330},"id":"mhAsbpcY-9xW","outputId":"1788a76c-3e92-4a95-a10b-25a3639c56b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Using native bfloat16 mixed precision (no gradient scaling)\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Using pad_token, but it is not set yet.\n"]},{"name":"stdout","output_type":"stream","text":["Set padding token to EOS token\n"]},{"ename":"ValueError","evalue":"Invalid pattern: '**' can only be an entire path component","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-6-cc62a622ad06\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-5-9368ebfab1be\u003e\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Load dataset with streaming and specific config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 212\u001b[0;31m     dataset = load_dataset(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;34m\"HuggingFaceTB/smollm-corpus\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;34m\"cosmopedia-v2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1773\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1774\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_auth_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1502\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1503\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1217\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                     ) from None\n\u001b[0;32m-\u003e 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m         raise FileNotFoundError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1201\u001b[0m                     \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                     \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1203\u001b[0;31m                 ).get_module()\n\u001b[0m\u001b[1;32m   1204\u001b[0m         except (\n\u001b[1;32m   1205\u001b[0m             \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 769\u001b[0;31m             \u001b[0;32melse\u001b[0m \u001b[0mget_data_patterns_in_dataset_repository\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhfh_dataset_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m         )\n\u001b[1;32m    771\u001b[0m         data_files = DataFilesDict.from_hf_repo(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mget_data_patterns_in_dataset_repository\u001b[0;34m(dataset_info, base_path)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_resolve_single_pattern_in_dataset_repository\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 662\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_data_files_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         raise EmptyDatasetError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36m_get_data_files_patterns\u001b[0;34m(pattern_resolver)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 223\u001b[0;31m                     \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                         \u001b[0mnon_empty_splits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36m_resolve_single_pattern_in_dataset_repository\u001b[0;34m(dataset_info, pattern, base_path, allowed_extensions)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 473\u001b[0;31m     \u001b[0mglob_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPurePath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPurePath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m     matched_paths = [\n\u001b[1;32m    475\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mallpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithdirs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 611\u001b[0;31m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mends_with_sep\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/utils.py\u001b[0m in \u001b[0;36mglob_translate\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"**\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 729\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    730\u001b[0m                 \u001b[0;34m\"Invalid pattern: '**' can only be an entire path component\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"]}],"source":["# Start the training\n","train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1yY0ywtwW1AgQmgD9A4AnSlbeVpmOX1m5"},"id":"YJdL8NSx--5K","outputId":"d4a5e3b2-8c9a-40ea-ab0a-45b134fa6dec"},"outputs":[],"source":["from datasets import list_datasets\n","\n","# List available datasets to check if the dataset exists\n","datasets = list_datasets()\n","print(datasets)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hc-7mBbZBY63"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOGQUt+11OhaeA+ySzb6hPZ","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}