{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3647,"status":"ok","timestamp":1739525644812,"user":{"displayName":"Ezhirko Arulmozhi","userId":"16774972310128518178"},"user_tz":-330},"id":"s9cpZmyTB1mY","outputId":"0ed9eb3b-3f0d-490b-aff1-8d06619b1ac4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1739525644812,"user":{"displayName":"Ezhirko Arulmozhi","userId":"16774972310128518178"},"user_tz":-330},"id":"PMD6tyE7B_CD"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/ERAV3/Session15')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":11428,"status":"ok","timestamp":1739525656235,"user":{"displayName":"Ezhirko Arulmozhi","userId":"16774972310128518178"},"user_tz":-330},"id":"qGYxBVB79uD9"},"outputs":[],"source":["! pip install transformers>=4.30.0\n","! pip install datasets>=2.12.0"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9593,"status":"ok","timestamp":1739525665824,"user":{"displayName":"Ezhirko Arulmozhi","userId":"16774972310128518178"},"user_tz":-330},"id":"d6fWAClW8Gsh"},"outputs":[],"source":["import os\n","import re\n","import time\n","import glob\n","import torch\n","import sys\n","from torch.cuda.amp import autocast, GradScaler\n","from transformers import AutoTokenizer\n","from datasets import load_dataset\n","from DeepSeek import DeepSeek\n","from torch.optim.lr_scheduler import LambdaLR"]},{"cell_type":"markdown","metadata":{"id":"4IhjWC-E97Bo"},"source":["#### Define the functions"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1739525665824,"user":{"displayName":"Ezhirko Arulmozhi","userId":"16774972310128518178"},"user_tz":-330},"id":"KGf3V27Z8tJC"},"outputs":[],"source":["def get_device():\n","    if torch.cuda.is_available():\n","        return torch.device(\"cuda\"), \"cuda\"\n","    elif torch.backends.mps.is_available():\n","        return torch.device(\"mps\"), \"mps\"\n","    else:\n","        return torch.device(\"cpu\"), \"cpu\"\n","\n","def load_checkpoint(checkpoint_path):\n","    \"\"\"Safely load checkpoint with error handling\"\"\"\n","    try:\n","        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","        return checkpoint\n","    except (RuntimeError, EOFError, Exception) as e:\n","        print(f\"\\nWarning: Failed to load checkpoint at {checkpoint_path}\")\n","        print(f\"Error: {str(e)}\")\n","        print(\"Starting training from scratch...\\n\")\n","        return None\n","\n","def get_lr_scheduler(optimizer, config):\n","    warmup_steps = config[\"optimizer\"][\"learning_rate_scheduler\"][\"lr_warmup_steps\"]\n","    decay_start = config[\"optimizer\"][\"learning_rate_scheduler\"][\"lr_decay_starting_step\"]\n","    decay_steps = config[\"optimizer\"][\"learning_rate_scheduler\"][\"lr_decay_steps\"]\n","    base_lr = config[\"optimizer\"][\"learning_rate_scheduler\"][\"learning_rate\"]\n","    min_lr = config[\"optimizer\"][\"learning_rate_scheduler\"][\"min_decay_lr\"]\n","\n","    def lr_lambda(step):\n","        if step < warmup_steps:\n","            return step / warmup_steps\n","        elif step < decay_start:\n","            return 1.0\n","        else:\n","            decay_ratio = (step - decay_start) / decay_steps\n","            decay_ratio = min(1.0, decay_ratio)\n","            return 1.0 - (1.0 - min_lr / base_lr) * decay_ratio\n","\n","    return LambdaLR(optimizer, lr_lambda)\n","\n","def get_latest_checkpoint(checkpoint_dir):\n","    \"\"\"Find the latest checkpoint in the directory based on step number.\"\"\"\n","    checkpoints = glob.glob(os.path.join(checkpoint_dir, \"step_*.pt\"))\n","    if not checkpoints:\n","        return None\n","\n","    # Extract step numbers and find the latest\n","    steps = [int(ckpt.split('step_')[-1].replace('.pt', '')) for ckpt in checkpoints]\n","    latest_checkpoint = checkpoints[steps.index(max(steps))]\n","    return latest_checkpoint\n","\n","def save_final_model(model, save_path):\n","    \"\"\"Save the final model in .pt format\"\"\"\n","    torch.save(model.state_dict(), save_path)\n","    print(f\"Saved final model to: {save_path}\")\n","\n","def is_valid_loss(loss_value):\n","    \"\"\"Check if loss value is valid\"\"\"\n","    return loss_value is not None and not torch.isnan(loss_value) and not torch.isinf(loss_value)\n","\n","def train():\n","    # Get the best available device\n","    device, device_name = get_device()\n","    print(f\"Using device: {device_name}\")\n","\n","    # Define configuration\n","    config = {\n","        \"checkpoints\": {\n","            \"checkpoint_interval\": 500,\n","            \"checkpoints_path\": \"checkpoints\",\n","            \"checkpoints_path_is_shared_file_system\": False,\n","            \"resume_checkpoint_path\": None,\n","            \"save_final_state\": False,\n","            \"save_initial_state\": False,\n","        },\n","        \"general\": {\n","            \"benchmark_csv_path\": None,\n","            \"consumed_train_samples\": None,\n","            \"ignore_sanity_checks\": True,\n","            \"project\": \"DeepSeek\",\n","            \"run\": \"deepseek-774M\",\n","            \"seed\": 8,\n","            \"step\": None,\n","        },\n","        \"logging\": {\n","            \"iteration_step_info_interval\": 1,\n","            \"log_level\": \"info\",\n","            \"log_level_replica\": \"info\",\n","        },\n","        \"model\": {\n","            \"ddp_bucket_cap_mb\": 25,\n","            \"dtype\": \"bfloat16\",\n","            \"init_method\": {\n","                \"std\": 0.041666666666666664,\n","            },\n","            \"make_vocab_size_divisible_by\": 1,\n","            \"model_config\": {\n","                \"bos_token_id\": 0,\n","                \"eos_token_id\": 0,\n","                \"hidden_act\": \"silu\",\n","                \"hidden_size\": 576,\n","                \"initializer_range\": 0.041666666666666664,\n","                \"intermediate_size\": 1536,\n","                \"is_llama_config\": True,\n","                \"max_position_embeddings\": 2048,\n","                \"num_attention_heads\": 9,\n","                \"num_hidden_layers\": 30,\n","                \"num_key_value_heads\": 3,\n","                \"pad_token_id\": None,\n","                \"pretraining_tp\": 1,\n","                \"rms_norm_eps\": 1.0e-05,\n","                \"rope_interleaved\": False,\n","                \"rope_scaling\": None,\n","                \"rope_theta\": 10000.0,\n","                \"tie_word_embeddings\": True,\n","                \"use_cache\": True,\n","                \"vocab_size\": 49152,\n","            },\n","        },\n","        \"optimizer\": {\n","            \"accumulate_grad_in_fp32\": True,\n","            \"clip_grad\": 1.0,\n","            \"learning_rate_scheduler\": {\n","                \"learning_rate\": 0.003,\n","                \"lr_decay_starting_step\": 1600000,\n","                \"lr_decay_steps\": 400000,\n","                \"lr_decay_style\": \"linear\",\n","                \"lr_warmup_steps\": 2000,\n","                \"lr_warmup_style\": \"linear\",\n","                \"min_decay_lr\": 0,\n","            },\n","            \"optimizer_factory\": {\n","                \"adam_beta1\": 0.9,\n","                \"adam_beta2\": 0.95,\n","                \"adam_eps\": 1.0e-08,\n","                \"name\": \"adamW\",\n","                \"torch_adam_is_fused\": True,\n","            },\n","            \"weight_decay\": 0.01,\n","            \"zero_stage\": 0,\n","        },\n","        \"parallelism\": {\n","            \"dp\": 64,\n","            \"expert_parallel_size\": 1,\n","            \"pp\": 1,\n","            \"pp_engine\": \"1f1b\",\n","            \"recompute_layer\": False,\n","            \"tp\": 1,\n","            \"tp_linear_async_communication\": True,\n","            \"tp_mode\": \"REDUCE_SCATTER\",\n","            \"tp_recompute_allgather\": True,\n","        },\n","        \"tokenizer\": {\n","            \"tokenizer_max_length\": None,\n","            \"tokenizer_name_or_path\": \"HuggingFaceTB/cosmo2-tokenizer\",\n","            \"tokenizer_revision\": None,\n","        },\n","        \"tokens\": {\n","            \"batch_accumulation_per_replica\": 2,\n","            \"limit_test_batches\": 0,\n","            \"limit_val_batches\": 0,\n","            \"micro_batch_size\": 5120,\n","            \"sequence_length\": 2048,\n","            \"train_steps\": 10000,\n","            \"val_check_interval\": 500,\n","        },\n","    }\n","\n","    # Create checkpoints directory if it doesn't exist\n","    os.makedirs(config['checkpoints']['checkpoints_path'], exist_ok=True)\n","\n","    # Load model configuration\n","    model_config = config[\"model\"][\"model_config\"]\n","\n","    # Set random seed\n","    torch.manual_seed(config[\"general\"][\"seed\"])\n","\n","    # Initialize model\n","    model = DeepSeek(model_config)\n","\n","    # Initialize mixed precision training\n","    use_amp = device_name != \"cpu\"\n","    dtype = torch.float32\n","    if use_amp:\n","        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n","            dtype = torch.bfloat16\n","            scaler = None\n","            print(\"Using native bfloat16 mixed precision (no gradient scaling)\")\n","        else:\n","            dtype = torch.float16\n","            scaler = GradScaler()\n","            print(\"Using float16 mixed precision with gradient scaling\")\n","    else:\n","        scaler = None\n","        print(\"Using full precision (float32)\")\n","\n","    # Move model to device and set dtype\n","    model = model.to(device)\n","    if dtype != torch.float32:\n","        model = model.to(dtype)\n","\n","    # Compute total number of parameters\n","    total_params = sum(p.numel() for p in model.parameters())\n","\n","    # Compute model size in MB\n","    model_size = total_params * 2 / (1024 ** 2)\n","\n","    # Print results\n","    print(f\"Total Number of Parameters: {total_params}\")\n","    print(f\"Model Size: {model_size:.2f} MB\")\n","\n","    # Initialize optimizer\n","    use_fused = config[\"optimizer\"][\"optimizer_factory\"][\"torch_adam_is_fused\"]\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(),\n","        lr=config[\"optimizer\"][\"learning_rate_scheduler\"][\"learning_rate\"],\n","        betas=(\n","            config[\"optimizer\"][\"optimizer_factory\"][\"adam_beta1\"],\n","            config[\"optimizer\"][\"optimizer_factory\"][\"adam_beta2\"]\n","        ),\n","        eps=config[\"optimizer\"][\"optimizer_factory\"][\"adam_eps\"],\n","        weight_decay=config[\"optimizer\"][\"weight_decay\"],\n","        fused=use_fused\n","    )\n","\n","    # Initialize scheduler\n","    scheduler = get_lr_scheduler(optimizer, config)\n","\n","    # Initialize step counter\n","    step = 1\n","\n","    # Try to load checkpoint if exists\n","    checkpoint_files = glob.glob(os.path.join(config['checkpoints']['checkpoints_path'], 'step_*.pt'))\n","    if checkpoint_files:\n","        checkpoint_files.sort(key=lambda x: int(re.search(r'step_(\\d+)', x).group(1)))\n","        latest_checkpoint = checkpoint_files[-1]\n","        print(f\"\\nFound checkpoint: {latest_checkpoint}\")\n","        checkpoint = load_checkpoint(latest_checkpoint)\n","\n","        if checkpoint is not None:\n","            try:\n","                model.load_state_dict(checkpoint['model_state_dict'])\n","                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","                step = checkpoint['step'] + 1  # Start from next step\n","                print(f\"Successfully resumed from step {step-1}\")\n","            except Exception as e:\n","                print(f\"Warning: Failed to restore checkpoint state: {str(e)}\")\n","                print(\"Starting training from scratch...\")\n","                step = 1\n","\n","    # Initialize tokenizer with padding token\n","    tokenizer = AutoTokenizer.from_pretrained(config[\"tokenizer\"][\"tokenizer_name_or_path\"])\n","\n","    # Set padding token to eos token if pad token is not set\n","    if tokenizer.pad_token is None:\n","        tokenizer.pad_token = tokenizer.eos_token\n","        print(\"Set padding token to EOS token\")\n","\n","    # Update model config with pad token id\n","    model_config[\"pad_token_id\"] = tokenizer.pad_token_id\n","\n","    # Load dataset with streaming and specific config\n","    dataset = load_dataset(\n","        \"HuggingFaceTB/smollm-corpus\",\n","        \"cosmopedia-v2\",\n","        streaming=True\n","    )\n","    train_dataset = dataset[\"train\"]\n","    print(\"Loaded cosmopedia-v2 dataset in streaming mode\")\n","\n","    def tokenize_function(examples):\n","        # Process the batch of texts\n","        tokenized = tokenizer(\n","            examples[\"text\"],\n","            truncation=True,\n","            max_length=config[\"tokens\"][\"sequence_length\"],\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        )\n","        return {\n","            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n","            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0)\n","        }\n","\n","    # Initialize dataset and iterator\n","    train_iter = iter(train_dataset.map(\n","        tokenize_function,\n","        remove_columns=train_dataset.column_names,\n","        batched=True,\n","        batch_size=config[\"tokens\"][\"micro_batch_size\"]\n","    ))\n","\n","    # Skip batches if resuming from checkpoint\n","    if step > 1:\n","        batches_to_skip = (step - 1) * config['tokens']['batch_accumulation_per_replica']\n","        print(f\"Skipping {batches_to_skip} batches to resume position...\")\n","        for _ in range(batches_to_skip):\n","            try:\n","                next(train_iter)\n","            except StopIteration:\n","                train_iter = iter(train_dataset.map(\n","                    tokenize_function,\n","                    remove_columns=train_dataset.column_names,\n","                    batched=True,\n","                    batch_size=config[\"tokens\"][\"micro_batch_size\"]\n","                ))\n","                next(train_iter)\n","        print(\"Done skipping batches\")\n","\n","    # Training parameters\n","    batch_size = config[\"tokens\"][\"micro_batch_size\"]\n","    accum_steps = config[\"tokens\"][\"batch_accumulation_per_replica\"]\n","    save_steps = 250\n","    eval_steps = 500\n","    grad_clip = 1.0\n","    initial_total_steps = config[\"tokens\"][\"train_steps\"]\n","    extended_steps = 100  # Additional steps beyond initial total\n","    final_total_steps = initial_total_steps + extended_steps\n","\n","    print(\"\\nStarting training...\")\n","    print(f\"Total steps: {final_total_steps}\")\n","    print(f\"Device: {device_name}\")\n","    print(f\"{'='*50}\\n\")\n","\n","    while step <= final_total_steps:  # Changed < to <= to include final step\n","        step_start_time = time.time()\n","        accumulated_loss = 0\n","\n","        # Training step\n","        for accum_step in range(accum_steps):\n","            try:\n","                batch = next(train_iter)\n","            except StopIteration:\n","                train_iter = iter(train_dataset.map(\n","                    tokenize_function,\n","                    remove_columns=train_dataset.column_names,\n","                    batched=True,\n","                    batch_size=config[\"tokens\"][\"micro_batch_size\"]\n","                ))\n","                batch = next(train_iter)\n","\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","\n","            if input_ids.dim() == 1:\n","                input_ids = input_ids.unsqueeze(0)\n","                attention_mask = attention_mask.unsqueeze(0)\n","\n","            if use_amp:\n","                with autocast(dtype=dtype):\n","                    outputs = model(input_ids, attention_mask)\n","                    loss = torch.nn.functional.cross_entropy(\n","                        outputs.view(-1, outputs.size(-1)),\n","                        input_ids.view(-1)\n","                    )\n","                    loss = loss / accum_steps\n","\n","                if scaler is not None:\n","                    scaler.scale(loss).backward()\n","                else:\n","                    loss.backward()\n","            else:\n","                outputs = model(input_ids, attention_mask)\n","                loss = torch.nn.functional.cross_entropy(\n","                    outputs.view(-1, outputs.size(-1)),\n","                    input_ids.view(-1)\n","                )\n","                loss = loss / accum_steps\n","                loss.backward()\n","\n","            accumulated_loss += loss.item() * accum_steps\n","\n","        # Optimizer step\n","        if scaler is not None:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            scaler.step(optimizer)\n","            scaler.update()\n","        else:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        # Get current learning rate\n","        current_lr = scheduler.get_last_lr()[0]\n","\n","        # Calculate step time and tokens/sec\n","        step_time = time.time() - step_start_time\n","        step_time_ms = step_time * 1000\n","        tokens_per_second = (config[\"tokens\"][\"micro_batch_size\"] * config[\"tokens\"][\"sequence_length\"]) / step_time\n","\n","        # Print step info\n","        print(f\"Step {step}/{final_total_steps} | Loss: {accumulated_loss:.4f} | LR: {current_lr:.6f} | \"\n","                    f\"Total Step Time: {step_time_ms:.2f}ms | \"\n","                    f\"Tokens/sec: {tokens_per_second:.2f} (accumulated over {config['tokens']['micro_batch_size']} batches)\")\n","\n","        # Text generation at step 500, 1000, 1500, etc.\n","        if (step % eval_steps) == 0:\n","            print(f\"\\n{'='*50}\")\n","            print(f\"Generating text sample at step {step}\")\n","            print(f\"{'='*50}\")\n","\n","            model.eval()\n","            with torch.no_grad():\n","                prompt = \"Once upon a time\"\n","                input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","                generated = model.generate(\n","                    input_ids,\n","                    max_length=200,\n","                    min_length=50,\n","                    num_return_sequences=1,\n","                    pad_token_id=tokenizer.pad_token_id,\n","                    do_sample=True,\n","                    temperature=0.8,\n","                    top_k=50,\n","                    top_p=0.95\n","                )\n","                generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n","                print(f\"Prompt: {prompt}\")\n","                print(f\"Generated text:\\n{generated_text}\")\n","                print(f\"{'='*50}\\n\")\n","            model.train()\n","\n","        # Checkpointing at step 1000, 2000, 3000, etc.\n","        if (step % save_steps) == 0:\n","            print(f\"\\n{'='*50}\")\n","            print(f\"Saving checkpoint at step {step}\")\n","            print(f\"{'='*50}\")\n","\n","            checkpoint_path = os.path.join(config['checkpoints']['checkpoints_path'], f\"step_{step}.pt\")\n","            final_checkpoint_path = os.path.join(config['checkpoints']['checkpoints_path'], f\"step_final_{step}.pt\")\n","            final_model_path = \"deepseek_final.pt\"\n","\n","            try:\n","                torch.save({\n","                    'step': step,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'scheduler_state_dict': scheduler.state_dict(),\n","                    'loss': accumulated_loss,\n","                    'config': config,\n","                }, checkpoint_path)\n","                print(f\"Checkpoint saved to: {checkpoint_path}\")\n","                print(f\"{'='*50}\\n\")\n","\n","            except Exception as e:\n","                print(f\"Warning: Failed to save checkpoint: {str(e)}\")\n","                print(f\"{'='*50}\\n\")\n","\n","        # Increment step counter at the end\n","        step += 1\n","\n","    print(\"\\nTraining completed!\")\n","\n","    # Save final checkpoint and model\n","    final_checkpoint_path = f\"{config['checkpoints']['checkpoints_path']}/step_final_{final_total_steps}.pt\"\n","    torch.save({\n","        'step': step,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'loss': accumulated_loss,\n","        'config': config,\n","    }, final_checkpoint_path)\n","    print(f\"Saved final checkpoint: {final_checkpoint_path}\")\n","\n","    # Save final model in .pt format\n","    model_save_path = \"deepseek_final.pt\"\n","    torch.save(model.state_dict(), model_save_path)\n","    print(f\"Saved final model to: {model_save_path}\")\n","\n","    print(\"\\nTraining completed successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["38b4de9a266e418fafae19e0d72b5169","d5ab2d56f3bb40c2a9b8ea36acb5b143","812e6283e5c2445ebdce2bd7bdb2988b","8c9eac82737d49bf875f5dd5000ac579","e5ac71df2ced49a7987ea643ee5fa5d7","580f8106800945c68116ba2c8d4c1b85","481db2f91e7a4afbb0b4cd5e045595d6","0c1bcf81a7d949e087fb090f3a926221","84ee6bb772844d389f25f0cba18d947d","ca220176f77f4149b45b911740125b4a","6af2cb121b0d4498a74891413f33806e","7a38e03952024b51b589093eb2e9fb39","dc8f7c005c1d452284380d6f3ebe6344","8c161cf383c844f48bb4ce08ed7c31e1","a5922b7c3ab64d82b5a6fe840c6b6090","0720fa09d2ff4362b348f759eeb0627a","0609eb4e4e634ec0b2403775f5316bdc","b98ee80ecb9f47f48f0fd8368a0f18b4","45623ec0539340f39e03c68d81293ca4","9fd4c51b397846b79e98b45b2a294462","ba7049bde9094dddb1f54f418ada3b6d","91d2a4efc4ef4b63a6b74297427105ae"]},"id":"mhAsbpcY-9xW","outputId":"6965329c-ab0f-4af0-f539-9a64a93efa2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Using native bfloat16 mixed precision (no gradient scaling)\n","Total Number of Parameters: 774168798\n","Model Size: 1476.61 MB\n","\n","Found checkpoint: checkpoints/step_2000.pt\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-7b573bfc00aa>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"]},{"output_type":"stream","name":"stdout","text":["Successfully resumed from step 2000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Set padding token to EOS token\n"]},{"output_type":"display_data","data":{"text/plain":["Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38b4de9a266e418fafae19e0d72b5169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a38e03952024b51b589093eb2e9fb39"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded cosmopedia-v2 dataset in streaming mode\n","Skipping 4000 batches to resume position...\n","Done skipping batches\n","\n","Starting training...\n","Total steps: 10100\n","Device: cuda\n","==================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-7b573bfc00aa>:349: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast(dtype=dtype):\n","/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Step 2001/10100 | Loss: 0.1100 | LR: 0.003000 | Total Step Time: 10352.06ms | Tokens/sec: 1012914.88 (accumulated over 5120 batches)\n","Step 2002/10100 | Loss: 0.1613 | LR: 0.003000 | Total Step Time: 7789.31ms | Tokens/sec: 1346173.40 (accumulated over 5120 batches)\n","Step 2003/10100 | Loss: 0.2072 | LR: 0.003000 | Total Step Time: 8453.88ms | Tokens/sec: 1240348.17 (accumulated over 5120 batches)\n","Step 2004/10100 | Loss: 0.2982 | LR: 0.003000 | Total Step Time: 7956.60ms | Tokens/sec: 1317869.33 (accumulated over 5120 batches)\n","Step 2005/10100 | Loss: 0.2436 | LR: 0.003000 | Total Step Time: 7673.48ms | Tokens/sec: 1366493.13 (accumulated over 5120 batches)\n","Step 2006/10100 | Loss: 0.2962 | LR: 0.003000 | Total Step Time: 7835.07ms | Tokens/sec: 1338310.24 (accumulated over 5120 batches)\n","Step 2007/10100 | Loss: 0.5801 | LR: 0.003000 | Total Step Time: 7587.44ms | Tokens/sec: 1381989.87 (accumulated over 5120 batches)\n","Step 2008/10100 | Loss: 0.2213 | LR: 0.003000 | Total Step Time: 7664.03ms | Tokens/sec: 1368177.84 (accumulated over 5120 batches)\n","Step 2009/10100 | Loss: 0.9816 | LR: 0.003000 | Total Step Time: 7613.29ms | Tokens/sec: 1377297.22 (accumulated over 5120 batches)\n","Step 2010/10100 | Loss: 0.0954 | LR: 0.003000 | Total Step Time: 7568.47ms | Tokens/sec: 1385453.55 (accumulated over 5120 batches)\n","Step 2011/10100 | Loss: 0.3119 | LR: 0.003000 | Total Step Time: 7771.73ms | Tokens/sec: 1349217.73 (accumulated over 5120 batches)\n","Step 2012/10100 | Loss: 0.3012 | LR: 0.003000 | Total Step Time: 7688.62ms | Tokens/sec: 1363803.44 (accumulated over 5120 batches)\n","Step 2013/10100 | Loss: 0.2395 | LR: 0.003000 | Total Step Time: 7571.75ms | Tokens/sec: 1384852.27 (accumulated over 5120 batches)\n","Step 2014/10100 | Loss: 0.7152 | LR: 0.003000 | Total Step Time: 7613.80ms | Tokens/sec: 1377203.81 (accumulated over 5120 batches)\n","Step 2015/10100 | Loss: 0.0528 | LR: 0.003000 | Total Step Time: 7669.69ms | Tokens/sec: 1367169.47 (accumulated over 5120 batches)\n","Step 2016/10100 | Loss: 0.4062 | LR: 0.003000 | Total Step Time: 7798.56ms | Tokens/sec: 1344577.07 (accumulated over 5120 batches)\n","Step 2017/10100 | Loss: 0.2442 | LR: 0.003000 | Total Step Time: 7641.58ms | Tokens/sec: 1372197.49 (accumulated over 5120 batches)\n","Step 2018/10100 | Loss: 0.4112 | LR: 0.003000 | Total Step Time: 7753.35ms | Tokens/sec: 1352417.15 (accumulated over 5120 batches)\n","Step 2019/10100 | Loss: 0.3861 | LR: 0.003000 | Total Step Time: 7744.49ms | Tokens/sec: 1353964.39 (accumulated over 5120 batches)\n","Step 2020/10100 | Loss: 0.5616 | LR: 0.003000 | Total Step Time: 7414.06ms | Tokens/sec: 1414306.52 (accumulated over 5120 batches)\n","Step 2021/10100 | Loss: 0.2378 | LR: 0.003000 | Total Step Time: 7755.52ms | Tokens/sec: 1352038.52 (accumulated over 5120 batches)\n","Step 2022/10100 | Loss: 0.2205 | LR: 0.003000 | Total Step Time: 7618.46ms | Tokens/sec: 1376361.39 (accumulated over 5120 batches)\n","Step 2023/10100 | Loss: 0.1501 | LR: 0.003000 | Total Step Time: 7752.02ms | Tokens/sec: 1352648.17 (accumulated over 5120 batches)\n","Step 2024/10100 | Loss: 0.3659 | LR: 0.003000 | Total Step Time: 7780.06ms | Tokens/sec: 1347774.24 (accumulated over 5120 batches)\n","Step 2025/10100 | Loss: 0.5575 | LR: 0.003000 | Total Step Time: 7431.44ms | Tokens/sec: 1411000.27 (accumulated over 5120 batches)\n","Step 2026/10100 | Loss: 0.1985 | LR: 0.003000 | Total Step Time: 7756.63ms | Tokens/sec: 1351844.15 (accumulated over 5120 batches)\n","Step 2027/10100 | Loss: 0.2887 | LR: 0.003000 | Total Step Time: 7620.95ms | Tokens/sec: 1375913.01 (accumulated over 5120 batches)\n","Step 2028/10100 | Loss: 0.3485 | LR: 0.003000 | Total Step Time: 7781.86ms | Tokens/sec: 1347462.23 (accumulated over 5120 batches)\n","Step 2029/10100 | Loss: 0.2884 | LR: 0.003000 | Total Step Time: 7744.95ms | Tokens/sec: 1353883.36 (accumulated over 5120 batches)\n","Step 2030/10100 | Loss: 0.1467 | LR: 0.003000 | Total Step Time: 7676.39ms | Tokens/sec: 1365975.17 (accumulated over 5120 batches)\n","Step 2031/10100 | Loss: 0.2257 | LR: 0.003000 | Total Step Time: 7805.59ms | Tokens/sec: 1343365.68 (accumulated over 5120 batches)\n","Step 2032/10100 | Loss: 0.3304 | LR: 0.003000 | Total Step Time: 7517.61ms | Tokens/sec: 1394827.11 (accumulated over 5120 batches)\n","Step 2033/10100 | Loss: 0.5970 | LR: 0.003000 | Total Step Time: 7812.58ms | Tokens/sec: 1342162.74 (accumulated over 5120 batches)\n","Step 2034/10100 | Loss: 0.3627 | LR: 0.003000 | Total Step Time: 7696.88ms | Tokens/sec: 1362339.09 (accumulated over 5120 batches)\n","Step 2035/10100 | Loss: 0.4600 | LR: 0.003000 | Total Step Time: 7549.43ms | Tokens/sec: 1388946.96 (accumulated over 5120 batches)\n","Step 2036/10100 | Loss: 0.1502 | LR: 0.003000 | Total Step Time: 7757.02ms | Tokens/sec: 1351777.26 (accumulated over 5120 batches)\n","Step 2037/10100 | Loss: 0.0951 | LR: 0.003000 | Total Step Time: 7552.66ms | Tokens/sec: 1388353.81 (accumulated over 5120 batches)\n","Step 2038/10100 | Loss: 0.2276 | LR: 0.003000 | Total Step Time: 7749.91ms | Tokens/sec: 1353016.94 (accumulated over 5120 batches)\n","Step 2039/10100 | Loss: 0.4592 | LR: 0.003000 | Total Step Time: 7574.02ms | Tokens/sec: 1384437.35 (accumulated over 5120 batches)\n","Step 2040/10100 | Loss: 0.4081 | LR: 0.003000 | Total Step Time: 7663.33ms | Tokens/sec: 1368303.28 (accumulated over 5120 batches)\n","Step 2041/10100 | Loss: 0.2557 | LR: 0.003000 | Total Step Time: 7755.77ms | Tokens/sec: 1351995.55 (accumulated over 5120 batches)\n","Step 2042/10100 | Loss: 0.1175 | LR: 0.003000 | Total Step Time: 7587.36ms | Tokens/sec: 1382003.73 (accumulated over 5120 batches)\n","Step 2043/10100 | Loss: 0.5171 | LR: 0.003000 | Total Step Time: 7736.02ms | Tokens/sec: 1355445.57 (accumulated over 5120 batches)\n","Step 2044/10100 | Loss: 0.3686 | LR: 0.003000 | Total Step Time: 7598.28ms | Tokens/sec: 1380017.50 (accumulated over 5120 batches)\n","Step 2045/10100 | Loss: 0.5708 | LR: 0.003000 | Total Step Time: 7681.06ms | Tokens/sec: 1365144.35 (accumulated over 5120 batches)\n","Step 2046/10100 | Loss: 0.2175 | LR: 0.003000 | Total Step Time: 7758.64ms | Tokens/sec: 1351495.12 (accumulated over 5120 batches)\n","Step 2047/10100 | Loss: 0.5784 | LR: 0.003000 | Total Step Time: 7471.86ms | Tokens/sec: 1403366.29 (accumulated over 5120 batches)\n","Step 2048/10100 | Loss: 0.2511 | LR: 0.003000 | Total Step Time: 7747.95ms | Tokens/sec: 1353359.80 (accumulated over 5120 batches)\n","Step 2049/10100 | Loss: 0.1368 | LR: 0.003000 | Total Step Time: 7568.28ms | Tokens/sec: 1385486.94 (accumulated over 5120 batches)\n","Step 2050/10100 | Loss: 0.4271 | LR: 0.003000 | Total Step Time: 7746.07ms | Tokens/sec: 1353687.42 (accumulated over 5120 batches)\n","Step 2051/10100 | Loss: 0.3586 | LR: 0.003000 | Total Step Time: 7681.21ms | Tokens/sec: 1365118.12 (accumulated over 5120 batches)\n","Step 2052/10100 | Loss: 0.2671 | LR: 0.003000 | Total Step Time: 7541.65ms | Tokens/sec: 1390379.95 (accumulated over 5120 batches)\n","Step 2053/10100 | Loss: 0.2242 | LR: 0.003000 | Total Step Time: 7799.03ms | Tokens/sec: 1344495.44 (accumulated over 5120 batches)\n","Step 2054/10100 | Loss: 0.4807 | LR: 0.003000 | Total Step Time: 7466.54ms | Tokens/sec: 1404366.48 (accumulated over 5120 batches)\n","Step 2055/10100 | Loss: 0.0981 | LR: 0.003000 | Total Step Time: 7845.92ms | Tokens/sec: 1336460.98 (accumulated over 5120 batches)\n","Step 2056/10100 | Loss: 0.2923 | LR: 0.003000 | Total Step Time: 7694.39ms | Tokens/sec: 1362779.25 (accumulated over 5120 batches)\n","Step 2057/10100 | Loss: 0.2890 | LR: 0.003000 | Total Step Time: 7661.68ms | Tokens/sec: 1368597.42 (accumulated over 5120 batches)\n","Step 2058/10100 | Loss: 0.1964 | LR: 0.003000 | Total Step Time: 7762.94ms | Tokens/sec: 1350745.58 (accumulated over 5120 batches)\n","Step 2059/10100 | Loss: 0.4059 | LR: 0.003000 | Total Step Time: 7547.93ms | Tokens/sec: 1389223.58 (accumulated over 5120 batches)\n","Step 2060/10100 | Loss: 0.4414 | LR: 0.003000 | Total Step Time: 7689.78ms | Tokens/sec: 1363596.54 (accumulated over 5120 batches)\n","Step 2061/10100 | Loss: 0.2943 | LR: 0.003000 | Total Step Time: 7661.12ms | Tokens/sec: 1368697.80 (accumulated over 5120 batches)\n","Step 2062/10100 | Loss: 0.6575 | LR: 0.003000 | Total Step Time: 7601.29ms | Tokens/sec: 1379470.51 (accumulated over 5120 batches)\n","Step 2063/10100 | Loss: 0.4302 | LR: 0.003000 | Total Step Time: 7722.36ms | Tokens/sec: 1357844.29 (accumulated over 5120 batches)\n","Step 2064/10100 | Loss: 0.6327 | LR: 0.003000 | Total Step Time: 7457.57ms | Tokens/sec: 1406056.25 (accumulated over 5120 batches)\n","Step 2065/10100 | Loss: 0.5008 | LR: 0.003000 | Total Step Time: 7733.20ms | Tokens/sec: 1355941.11 (accumulated over 5120 batches)\n","Step 2066/10100 | Loss: 0.2191 | LR: 0.003000 | Total Step Time: 7566.07ms | Tokens/sec: 1385892.66 (accumulated over 5120 batches)\n","Step 2067/10100 | Loss: 0.3095 | LR: 0.003000 | Total Step Time: 7678.93ms | Tokens/sec: 1365524.08 (accumulated over 5120 batches)\n","Step 2068/10100 | Loss: 0.3498 | LR: 0.003000 | Total Step Time: 7756.08ms | Tokens/sec: 1351940.19 (accumulated over 5120 batches)\n","Step 2069/10100 | Loss: 0.1697 | LR: 0.003000 | Total Step Time: 7592.85ms | Tokens/sec: 1381004.16 (accumulated over 5120 batches)\n","Step 2070/10100 | Loss: 0.1962 | LR: 0.003000 | Total Step Time: 7802.93ms | Tokens/sec: 1343822.81 (accumulated over 5120 batches)\n","Step 2071/10100 | Loss: 0.4104 | LR: 0.003000 | Total Step Time: 7512.21ms | Tokens/sec: 1395829.43 (accumulated over 5120 batches)\n","Step 2072/10100 | Loss: 0.3970 | LR: 0.003000 | Total Step Time: 7791.59ms | Tokens/sec: 1345779.24 (accumulated over 5120 batches)\n","Step 2073/10100 | Loss: 0.2354 | LR: 0.003000 | Total Step Time: 7696.86ms | Tokens/sec: 1362341.75 (accumulated over 5120 batches)\n","Step 2074/10100 | Loss: 0.3201 | LR: 0.003000 | Total Step Time: 7586.68ms | Tokens/sec: 1382128.16 (accumulated over 5120 batches)\n","Step 2075/10100 | Loss: 0.8627 | LR: 0.003000 | Total Step Time: 7690.81ms | Tokens/sec: 1363413.97 (accumulated over 5120 batches)\n","Step 2076/10100 | Loss: 0.4526 | LR: 0.003000 | Total Step Time: 7512.83ms | Tokens/sec: 1395713.42 (accumulated over 5120 batches)\n","Step 2077/10100 | Loss: 0.3581 | LR: 0.003000 | Total Step Time: 7803.64ms | Tokens/sec: 1343701.57 (accumulated over 5120 batches)\n","Step 2078/10100 | Loss: 0.0906 | LR: 0.003000 | Total Step Time: 7747.01ms | Tokens/sec: 1353524.28 (accumulated over 5120 batches)\n","Step 2079/10100 | Loss: 0.4928 | LR: 0.003000 | Total Step Time: 7609.56ms | Tokens/sec: 1377972.74 (accumulated over 5120 batches)\n","Step 2080/10100 | Loss: 0.1413 | LR: 0.003000 | Total Step Time: 7788.19ms | Tokens/sec: 1346366.14 (accumulated over 5120 batches)\n","Step 2081/10100 | Loss: 0.1863 | LR: 0.003000 | Total Step Time: 7615.88ms | Tokens/sec: 1376827.81 (accumulated over 5120 batches)\n","Step 2082/10100 | Loss: 0.2087 | LR: 0.003000 | Total Step Time: 7812.73ms | Tokens/sec: 1342137.88 (accumulated over 5120 batches)\n","Step 2083/10100 | Loss: 0.2616 | LR: 0.003000 | Total Step Time: 7589.38ms | Tokens/sec: 1381636.13 (accumulated over 5120 batches)\n","Step 2084/10100 | Loss: 0.2139 | LR: 0.003000 | Total Step Time: 7692.66ms | Tokens/sec: 1363087.24 (accumulated over 5120 batches)\n","Step 2085/10100 | Loss: 0.4394 | LR: 0.003000 | Total Step Time: 7729.44ms | Tokens/sec: 1356600.73 (accumulated over 5120 batches)\n","Step 2086/10100 | Loss: 0.3245 | LR: 0.003000 | Total Step Time: 7520.78ms | Tokens/sec: 1394237.82 (accumulated over 5120 batches)\n","Step 2087/10100 | Loss: 0.2753 | LR: 0.003000 | Total Step Time: 7766.43ms | Tokens/sec: 1350138.18 (accumulated over 5120 batches)\n","Step 2088/10100 | Loss: 0.2776 | LR: 0.003000 | Total Step Time: 7625.90ms | Tokens/sec: 1375018.69 (accumulated over 5120 batches)\n","Step 2089/10100 | Loss: 0.3480 | LR: 0.003000 | Total Step Time: 7735.93ms | Tokens/sec: 1355461.70 (accumulated over 5120 batches)\n","Step 2090/10100 | Loss: 0.2458 | LR: 0.003000 | Total Step Time: 7765.07ms | Tokens/sec: 1350375.55 (accumulated over 5120 batches)\n","Step 2091/10100 | Loss: 0.1841 | LR: 0.003000 | Total Step Time: 7587.30ms | Tokens/sec: 1382013.80 (accumulated over 5120 batches)\n","Step 2092/10100 | Loss: 0.2973 | LR: 0.003000 | Total Step Time: 7739.27ms | Tokens/sec: 1354877.39 (accumulated over 5120 batches)\n","Step 2093/10100 | Loss: 0.3570 | LR: 0.003000 | Total Step Time: 7585.22ms | Tokens/sec: 1382394.25 (accumulated over 5120 batches)\n","Step 2094/10100 | Loss: 0.0736 | LR: 0.003000 | Total Step Time: 7758.06ms | Tokens/sec: 1351596.38 (accumulated over 5120 batches)\n","Step 2095/10100 | Loss: 0.2583 | LR: 0.003000 | Total Step Time: 7745.93ms | Tokens/sec: 1353711.76 (accumulated over 5120 batches)\n","Step 2096/10100 | Loss: 0.2270 | LR: 0.003000 | Total Step Time: 7528.29ms | Tokens/sec: 1392848.39 (accumulated over 5120 batches)\n","Step 2097/10100 | Loss: 0.5347 | LR: 0.003000 | Total Step Time: 7719.32ms | Tokens/sec: 1358378.08 (accumulated over 5120 batches)\n","Step 2098/10100 | Loss: 0.2423 | LR: 0.003000 | Total Step Time: 7607.06ms | Tokens/sec: 1378425.65 (accumulated over 5120 batches)\n","Step 2099/10100 | Loss: 0.3024 | LR: 0.003000 | Total Step Time: 7762.23ms | Tokens/sec: 1350869.21 (accumulated over 5120 batches)\n","Step 2100/10100 | Loss: 0.1230 | LR: 0.003000 | Total Step Time: 7782.91ms | Tokens/sec: 1347280.03 (accumulated over 5120 batches)\n","Step 2101/10100 | Loss: 0.2139 | LR: 0.003000 | Total Step Time: 7617.24ms | Tokens/sec: 1376582.09 (accumulated over 5120 batches)\n","Step 2102/10100 | Loss: 0.0721 | LR: 0.003000 | Total Step Time: 7797.83ms | Tokens/sec: 1344701.88 (accumulated over 5120 batches)\n","Step 2103/10100 | Loss: 0.2303 | LR: 0.003000 | Total Step Time: 7515.72ms | Tokens/sec: 1395176.13 (accumulated over 5120 batches)\n","Step 2104/10100 | Loss: 0.2592 | LR: 0.003000 | Total Step Time: 7767.32ms | Tokens/sec: 1349983.64 (accumulated over 5120 batches)\n","Step 2105/10100 | Loss: 0.2142 | LR: 0.003000 | Total Step Time: 7687.29ms | Tokens/sec: 1364038.19 (accumulated over 5120 batches)\n","Step 2106/10100 | Loss: 0.5224 | LR: 0.003000 | Total Step Time: 7620.34ms | Tokens/sec: 1376023.13 (accumulated over 5120 batches)\n","Step 2107/10100 | Loss: 0.7850 | LR: 0.003000 | Total Step Time: 7718.95ms | Tokens/sec: 1358443.49 (accumulated over 5120 batches)\n","Step 2108/10100 | Loss: 0.1429 | LR: 0.003000 | Total Step Time: 7645.91ms | Tokens/sec: 1371420.88 (accumulated over 5120 batches)\n","Step 2109/10100 | Loss: 0.2816 | LR: 0.003000 | Total Step Time: 7760.80ms | Tokens/sec: 1351117.88 (accumulated over 5120 batches)\n","Step 2110/10100 | Loss: 0.2594 | LR: 0.003000 | Total Step Time: 7699.76ms | Tokens/sec: 1361829.51 (accumulated over 5120 batches)\n","Step 2111/10100 | Loss: 0.3617 | LR: 0.003000 | Total Step Time: 7660.49ms | Tokens/sec: 1368810.39 (accumulated over 5120 batches)\n","Step 2112/10100 | Loss: 0.3036 | LR: 0.003000 | Total Step Time: 7713.79ms | Tokens/sec: 1359353.09 (accumulated over 5120 batches)\n","Step 2113/10100 | Loss: 0.3592 | LR: 0.003000 | Total Step Time: 7551.17ms | Tokens/sec: 1388627.96 (accumulated over 5120 batches)\n","Step 2114/10100 | Loss: 0.2068 | LR: 0.003000 | Total Step Time: 7821.92ms | Tokens/sec: 1340561.06 (accumulated over 5120 batches)\n","Step 2115/10100 | Loss: 0.2760 | LR: 0.003000 | Total Step Time: 7597.77ms | Tokens/sec: 1380111.04 (accumulated over 5120 batches)\n","Step 2116/10100 | Loss: 0.6147 | LR: 0.003000 | Total Step Time: 7692.04ms | Tokens/sec: 1363196.58 (accumulated over 5120 batches)\n","Step 2117/10100 | Loss: 0.3577 | LR: 0.003000 | Total Step Time: 7798.14ms | Tokens/sec: 1344648.72 (accumulated over 5120 batches)\n","Step 2118/10100 | Loss: 0.4240 | LR: 0.003000 | Total Step Time: 7437.44ms | Tokens/sec: 1409861.07 (accumulated over 5120 batches)\n","Step 2119/10100 | Loss: 0.4506 | LR: 0.003000 | Total Step Time: 7709.42ms | Tokens/sec: 1360122.19 (accumulated over 5120 batches)\n","Step 2120/10100 | Loss: 0.2050 | LR: 0.003000 | Total Step Time: 7613.31ms | Tokens/sec: 1377293.52 (accumulated over 5120 batches)\n","Step 2121/10100 | Loss: 0.3123 | LR: 0.003000 | Total Step Time: 7754.61ms | Tokens/sec: 1352196.52 (accumulated over 5120 batches)\n","Step 2122/10100 | Loss: 0.4924 | LR: 0.003000 | Total Step Time: 7768.62ms | Tokens/sec: 1349758.30 (accumulated over 5120 batches)\n","Step 2123/10100 | Loss: 0.4092 | LR: 0.003000 | Total Step Time: 7572.53ms | Tokens/sec: 1384709.73 (accumulated over 5120 batches)\n","Step 2124/10100 | Loss: 0.5789 | LR: 0.003000 | Total Step Time: 7673.74ms | Tokens/sec: 1366447.87 (accumulated over 5120 batches)\n","Step 2125/10100 | Loss: 0.2469 | LR: 0.003000 | Total Step Time: 7605.74ms | Tokens/sec: 1378663.74 (accumulated over 5120 batches)\n","Step 2126/10100 | Loss: 0.2482 | LR: 0.003000 | Total Step Time: 7823.76ms | Tokens/sec: 1340244.79 (accumulated over 5120 batches)\n","Step 2127/10100 | Loss: 0.1886 | LR: 0.003000 | Total Step Time: 7772.20ms | Tokens/sec: 1349135.82 (accumulated over 5120 batches)\n","Step 2128/10100 | Loss: 0.5522 | LR: 0.003000 | Total Step Time: 7587.16ms | Tokens/sec: 1382040.60 (accumulated over 5120 batches)\n","Step 2129/10100 | Loss: 0.3513 | LR: 0.003000 | Total Step Time: 7769.08ms | Tokens/sec: 1349679.10 (accumulated over 5120 batches)\n","Step 2130/10100 | Loss: 0.1849 | LR: 0.003000 | Total Step Time: 7559.71ms | Tokens/sec: 1387057.92 (accumulated over 5120 batches)\n","Step 2131/10100 | Loss: 0.5499 | LR: 0.003000 | Total Step Time: 7715.77ms | Tokens/sec: 1359003.11 (accumulated over 5120 batches)\n","Step 2132/10100 | Loss: 0.4434 | LR: 0.003000 | Total Step Time: 7681.79ms | Tokens/sec: 1365014.44 (accumulated over 5120 batches)\n","Step 2133/10100 | Loss: 0.2526 | LR: 0.003000 | Total Step Time: 7512.64ms | Tokens/sec: 1395748.37 (accumulated over 5120 batches)\n","Step 2134/10100 | Loss: 0.2899 | LR: 0.003000 | Total Step Time: 7796.51ms | Tokens/sec: 1344929.81 (accumulated over 5120 batches)\n","Step 2135/10100 | Loss: 0.1833 | LR: 0.003000 | Total Step Time: 7537.51ms | Tokens/sec: 1391144.17 (accumulated over 5120 batches)\n","Step 2136/10100 | Loss: 0.4312 | LR: 0.003000 | Total Step Time: 7654.26ms | Tokens/sec: 1369925.33 (accumulated over 5120 batches)\n","Step 2137/10100 | Loss: 0.3597 | LR: 0.003000 | Total Step Time: 7593.21ms | Tokens/sec: 1380939.55 (accumulated over 5120 batches)\n","Step 2138/10100 | Loss: 0.4268 | LR: 0.003000 | Total Step Time: 7626.59ms | Tokens/sec: 1374894.34 (accumulated over 5120 batches)\n","Step 2139/10100 | Loss: 0.5795 | LR: 0.003000 | Total Step Time: 7561.56ms | Tokens/sec: 1386719.20 (accumulated over 5120 batches)\n","Step 2140/10100 | Loss: 0.1503 | LR: 0.003000 | Total Step Time: 7576.34ms | Tokens/sec: 1384014.58 (accumulated over 5120 batches)\n","Step 2141/10100 | Loss: 0.1412 | LR: 0.003000 | Total Step Time: 7840.15ms | Tokens/sec: 1337443.45 (accumulated over 5120 batches)\n","Step 2142/10100 | Loss: 0.2779 | LR: 0.003000 | Total Step Time: 7610.53ms | Tokens/sec: 1377796.78 (accumulated over 5120 batches)\n","Step 2143/10100 | Loss: 0.6693 | LR: 0.003000 | Total Step Time: 7665.25ms | Tokens/sec: 1367959.82 (accumulated over 5120 batches)\n","Step 2144/10100 | Loss: 0.3291 | LR: 0.003000 | Total Step Time: 7668.02ms | Tokens/sec: 1367466.69 (accumulated over 5120 batches)\n","Step 2145/10100 | Loss: 0.2409 | LR: 0.003000 | Total Step Time: 7612.80ms | Tokens/sec: 1377385.56 (accumulated over 5120 batches)\n","Step 2146/10100 | Loss: 0.2618 | LR: 0.003000 | Total Step Time: 7694.46ms | Tokens/sec: 1362768.02 (accumulated over 5120 batches)\n","Step 2147/10100 | Loss: 0.1470 | LR: 0.003000 | Total Step Time: 7571.60ms | Tokens/sec: 1384881.18 (accumulated over 5120 batches)\n","Step 2148/10100 | Loss: 0.5902 | LR: 0.003000 | Total Step Time: 7642.77ms | Tokens/sec: 1371984.49 (accumulated over 5120 batches)\n","Step 2149/10100 | Loss: 0.3856 | LR: 0.003000 | Total Step Time: 7637.01ms | Tokens/sec: 1373018.92 (accumulated over 5120 batches)\n","Step 2150/10100 | Loss: 0.1912 | LR: 0.003000 | Total Step Time: 7677.10ms | Tokens/sec: 1365849.09 (accumulated over 5120 batches)\n","Step 2151/10100 | Loss: 0.1311 | LR: 0.003000 | Total Step Time: 7817.70ms | Tokens/sec: 1341285.07 (accumulated over 5120 batches)\n","Step 2152/10100 | Loss: 0.5094 | LR: 0.003000 | Total Step Time: 7549.98ms | Tokens/sec: 1388846.30 (accumulated over 5120 batches)\n","Step 2153/10100 | Loss: 0.2242 | LR: 0.003000 | Total Step Time: 7832.53ms | Tokens/sec: 1338744.74 (accumulated over 5120 batches)\n","Step 2154/10100 | Loss: 0.3367 | LR: 0.003000 | Total Step Time: 7586.77ms | Tokens/sec: 1382111.13 (accumulated over 5120 batches)\n","Step 2155/10100 | Loss: 0.2287 | LR: 0.003000 | Total Step Time: 7697.04ms | Tokens/sec: 1362309.97 (accumulated over 5120 batches)\n","Step 2156/10100 | Loss: 0.1164 | LR: 0.003000 | Total Step Time: 7820.92ms | Tokens/sec: 1340732.78 (accumulated over 5120 batches)\n","Step 2157/10100 | Loss: 0.2057 | LR: 0.003000 | Total Step Time: 7562.14ms | Tokens/sec: 1386612.92 (accumulated over 5120 batches)\n","Step 2158/10100 | Loss: 0.2345 | LR: 0.003000 | Total Step Time: 7767.67ms | Tokens/sec: 1349924.10 (accumulated over 5120 batches)\n","Step 2159/10100 | Loss: 0.3343 | LR: 0.003000 | Total Step Time: 7547.37ms | Tokens/sec: 1389326.49 (accumulated over 5120 batches)\n","Step 2160/10100 | Loss: 0.2132 | LR: 0.003000 | Total Step Time: 7758.22ms | Tokens/sec: 1351568.22 (accumulated over 5120 batches)\n","Step 2161/10100 | Loss: 0.1777 | LR: 0.003000 | Total Step Time: 7754.88ms | Tokens/sec: 1352149.26 (accumulated over 5120 batches)\n","Step 2162/10100 | Loss: 0.2853 | LR: 0.003000 | Total Step Time: 7570.45ms | Tokens/sec: 1385091.57 (accumulated over 5120 batches)\n","Step 2163/10100 | Loss: 0.3039 | LR: 0.003000 | Total Step Time: 7731.04ms | Tokens/sec: 1356318.71 (accumulated over 5120 batches)\n","Step 2164/10100 | Loss: 0.2870 | LR: 0.003000 | Total Step Time: 7570.90ms | Tokens/sec: 1385008.13 (accumulated over 5120 batches)\n","Step 2165/10100 | Loss: 0.2098 | LR: 0.003000 | Total Step Time: 7774.43ms | Tokens/sec: 1348749.14 (accumulated over 5120 batches)\n","Step 2166/10100 | Loss: 0.2328 | LR: 0.003000 | Total Step Time: 7782.80ms | Tokens/sec: 1347299.34 (accumulated over 5120 batches)\n","Step 2167/10100 | Loss: 0.3351 | LR: 0.003000 | Total Step Time: 7503.86ms | Tokens/sec: 1397381.62 (accumulated over 5120 batches)\n","Step 2168/10100 | Loss: 0.3110 | LR: 0.003000 | Total Step Time: 7757.14ms | Tokens/sec: 1351756.61 (accumulated over 5120 batches)\n","Step 2169/10100 | Loss: 0.1286 | LR: 0.003000 | Total Step Time: 7626.82ms | Tokens/sec: 1374854.28 (accumulated over 5120 batches)\n","Step 2170/10100 | Loss: 0.1634 | LR: 0.003000 | Total Step Time: 7758.62ms | Tokens/sec: 1351497.78 (accumulated over 5120 batches)\n","Step 2171/10100 | Loss: 0.4882 | LR: 0.003000 | Total Step Time: 7583.10ms | Tokens/sec: 1382779.98 (accumulated over 5120 batches)\n","Step 2172/10100 | Loss: 0.3197 | LR: 0.003000 | Total Step Time: 7571.33ms | Tokens/sec: 1384929.89 (accumulated over 5120 batches)\n","Step 2173/10100 | Loss: 0.2470 | LR: 0.003000 | Total Step Time: 7806.04ms | Tokens/sec: 1343288.63 (accumulated over 5120 batches)\n","Step 2174/10100 | Loss: 0.6861 | LR: 0.003000 | Total Step Time: 7456.93ms | Tokens/sec: 1406176.46 (accumulated over 5120 batches)\n","Step 2175/10100 | Loss: 0.5128 | LR: 0.003000 | Total Step Time: 7686.63ms | Tokens/sec: 1364155.26 (accumulated over 5120 batches)\n","Step 2176/10100 | Loss: 0.9053 | LR: 0.003000 | Total Step Time: 7443.81ms | Tokens/sec: 1408655.66 (accumulated over 5120 batches)\n","Step 2177/10100 | Loss: 0.2367 | LR: 0.003000 | Total Step Time: 7679.41ms | Tokens/sec: 1365437.56 (accumulated over 5120 batches)\n","Step 2178/10100 | Loss: 0.3828 | LR: 0.003000 | Total Step Time: 7734.95ms | Tokens/sec: 1355634.00 (accumulated over 5120 batches)\n","Step 2179/10100 | Loss: 0.1161 | LR: 0.003000 | Total Step Time: 7548.79ms | Tokens/sec: 1389064.17 (accumulated over 5120 batches)\n","Step 2180/10100 | Loss: 0.2210 | LR: 0.003000 | Total Step Time: 7816.11ms | Tokens/sec: 1341557.39 (accumulated over 5120 batches)\n","Step 2181/10100 | Loss: 0.1704 | LR: 0.003000 | Total Step Time: 7585.08ms | Tokens/sec: 1382419.06 (accumulated over 5120 batches)\n","Step 2182/10100 | Loss: 0.4971 | LR: 0.003000 | Total Step Time: 7638.78ms | Tokens/sec: 1372700.43 (accumulated over 5120 batches)\n","Step 2183/10100 | Loss: 0.1792 | LR: 0.003000 | Total Step Time: 7800.27ms | Tokens/sec: 1344281.08 (accumulated over 5120 batches)\n","Step 2184/10100 | Loss: 0.3431 | LR: 0.003000 | Total Step Time: 7554.26ms | Tokens/sec: 1388058.44 (accumulated over 5120 batches)\n","Step 2185/10100 | Loss: 0.2118 | LR: 0.003000 | Total Step Time: 7602.63ms | Tokens/sec: 1379227.87 (accumulated over 5120 batches)\n","Step 2186/10100 | Loss: 0.5186 | LR: 0.003000 | Total Step Time: 7489.80ms | Tokens/sec: 1400004.81 (accumulated over 5120 batches)\n","Step 2187/10100 | Loss: 0.3459 | LR: 0.003000 | Total Step Time: 7666.19ms | Tokens/sec: 1367792.12 (accumulated over 5120 batches)\n","Step 2188/10100 | Loss: 0.4361 | LR: 0.003000 | Total Step Time: 7628.54ms | Tokens/sec: 1374543.96 (accumulated over 5120 batches)\n","Step 2189/10100 | Loss: 0.2508 | LR: 0.003000 | Total Step Time: 7543.02ms | Tokens/sec: 1390128.17 (accumulated over 5120 batches)\n","Step 2190/10100 | Loss: 0.4871 | LR: 0.003000 | Total Step Time: 7563.69ms | Tokens/sec: 1386328.33 (accumulated over 5120 batches)\n","Step 2191/10100 | Loss: 0.1413 | LR: 0.003000 | Total Step Time: 7485.58ms | Tokens/sec: 1400793.80 (accumulated over 5120 batches)\n","Step 2192/10100 | Loss: 0.3529 | LR: 0.003000 | Total Step Time: 7661.61ms | Tokens/sec: 1368610.58 (accumulated over 5120 batches)\n","Step 2193/10100 | Loss: 0.2156 | LR: 0.003000 | Total Step Time: 7617.06ms | Tokens/sec: 1376614.70 (accumulated over 5120 batches)\n","Step 2194/10100 | Loss: 0.3152 | LR: 0.003000 | Total Step Time: 7546.46ms | Tokens/sec: 1389494.38 (accumulated over 5120 batches)\n","Step 2195/10100 | Loss: 0.4207 | LR: 0.003000 | Total Step Time: 7595.98ms | Tokens/sec: 1380434.89 (accumulated over 5120 batches)\n","Step 2196/10100 | Loss: 0.2144 | LR: 0.003000 | Total Step Time: 7504.70ms | Tokens/sec: 1397225.35 (accumulated over 5120 batches)\n","Step 2197/10100 | Loss: 0.2962 | LR: 0.003000 | Total Step Time: 7705.97ms | Tokens/sec: 1360731.83 (accumulated over 5120 batches)\n","Step 2198/10100 | Loss: 0.3783 | LR: 0.003000 | Total Step Time: 7461.48ms | Tokens/sec: 1405318.35 (accumulated over 5120 batches)\n","Step 2199/10100 | Loss: 0.3033 | LR: 0.003000 | Total Step Time: 7668.20ms | Tokens/sec: 1367433.74 (accumulated over 5120 batches)\n","Step 2200/10100 | Loss: 0.1769 | LR: 0.003000 | Total Step Time: 7676.85ms | Tokens/sec: 1365892.87 (accumulated over 5120 batches)\n","Step 2201/10100 | Loss: 0.3644 | LR: 0.003000 | Total Step Time: 7444.34ms | Tokens/sec: 1408555.32 (accumulated over 5120 batches)\n","Step 2202/10100 | Loss: 0.2064 | LR: 0.003000 | Total Step Time: 7709.81ms | Tokens/sec: 1360053.68 (accumulated over 5120 batches)\n","Step 2203/10100 | Loss: 0.3143 | LR: 0.003000 | Total Step Time: 7447.00ms | Tokens/sec: 1408051.33 (accumulated over 5120 batches)\n","Step 2204/10100 | Loss: 0.2445 | LR: 0.003000 | Total Step Time: 7649.86ms | Tokens/sec: 1370712.09 (accumulated over 5120 batches)\n","Step 2205/10100 | Loss: 0.4473 | LR: 0.003000 | Total Step Time: 7515.82ms | Tokens/sec: 1395158.74 (accumulated over 5120 batches)\n","Step 2206/10100 | Loss: 0.3159 | LR: 0.003000 | Total Step Time: 7538.75ms | Tokens/sec: 1390914.78 (accumulated over 5120 batches)\n","Step 2207/10100 | Loss: 0.2010 | LR: 0.003000 | Total Step Time: 7656.44ms | Tokens/sec: 1369535.43 (accumulated over 5120 batches)\n","Step 2208/10100 | Loss: 0.2817 | LR: 0.003000 | Total Step Time: 7456.67ms | Tokens/sec: 1406226.23 (accumulated over 5120 batches)\n","Step 2209/10100 | Loss: 0.2388 | LR: 0.003000 | Total Step Time: 7677.09ms | Tokens/sec: 1365851.34 (accumulated over 5120 batches)\n","Step 2210/10100 | Loss: 0.4133 | LR: 0.003000 | Total Step Time: 7456.41ms | Tokens/sec: 1406275.20 (accumulated over 5120 batches)\n","Step 2211/10100 | Loss: 0.4913 | LR: 0.003000 | Total Step Time: 7556.27ms | Tokens/sec: 1387690.29 (accumulated over 5120 batches)\n","Step 2212/10100 | Loss: 0.1553 | LR: 0.003000 | Total Step Time: 7727.74ms | Tokens/sec: 1356899.36 (accumulated over 5120 batches)\n","Step 2213/10100 | Loss: 0.2985 | LR: 0.003000 | Total Step Time: 7455.12ms | Tokens/sec: 1406518.06 (accumulated over 5120 batches)\n","Step 2214/10100 | Loss: 0.2781 | LR: 0.003000 | Total Step Time: 7674.68ms | Tokens/sec: 1366280.49 (accumulated over 5120 batches)\n","Step 2215/10100 | Loss: 0.2828 | LR: 0.003000 | Total Step Time: 7403.42ms | Tokens/sec: 1416340.48 (accumulated over 5120 batches)\n","Step 2216/10100 | Loss: 0.8739 | LR: 0.003000 | Total Step Time: 7613.24ms | Tokens/sec: 1377306.54 (accumulated over 5120 batches)\n","Step 2217/10100 | Loss: 0.2490 | LR: 0.003000 | Total Step Time: 7597.10ms | Tokens/sec: 1380232.06 (accumulated over 5120 batches)\n","Step 2218/10100 | Loss: 0.2764 | LR: 0.003000 | Total Step Time: 7493.91ms | Tokens/sec: 1399237.99 (accumulated over 5120 batches)\n","Step 2219/10100 | Loss: 0.3548 | LR: 0.003000 | Total Step Time: 7672.13ms | Tokens/sec: 1366733.22 (accumulated over 5120 batches)\n","Step 2220/10100 | Loss: 0.2764 | LR: 0.003000 | Total Step Time: 7494.82ms | Tokens/sec: 1399066.93 (accumulated over 5120 batches)\n","Step 2221/10100 | Loss: 0.3809 | LR: 0.003000 | Total Step Time: 7665.34ms | Tokens/sec: 1367943.66 (accumulated over 5120 batches)\n","Step 2222/10100 | Loss: 0.2094 | LR: 0.003000 | Total Step Time: 7510.72ms | Tokens/sec: 1396106.76 (accumulated over 5120 batches)\n","Step 2223/10100 | Loss: 0.1930 | LR: 0.003000 | Total Step Time: 7664.32ms | Tokens/sec: 1368127.02 (accumulated over 5120 batches)\n","Step 2224/10100 | Loss: 0.5369 | LR: 0.003000 | Total Step Time: 7621.64ms | Tokens/sec: 1375787.59 (accumulated over 5120 batches)\n","Step 2225/10100 | Loss: 0.2733 | LR: 0.003000 | Total Step Time: 7447.84ms | Tokens/sec: 1407892.76 (accumulated over 5120 batches)\n","Step 2226/10100 | Loss: 0.2147 | LR: 0.003000 | Total Step Time: 7623.23ms | Tokens/sec: 1375501.11 (accumulated over 5120 batches)\n","Step 2227/10100 | Loss: 0.4855 | LR: 0.003000 | Total Step Time: 7396.28ms | Tokens/sec: 1417707.00 (accumulated over 5120 batches)\n","Step 2228/10100 | Loss: 0.5435 | LR: 0.003000 | Total Step Time: 7599.38ms | Tokens/sec: 1379818.26 (accumulated over 5120 batches)\n","Step 2229/10100 | Loss: 0.0940 | LR: 0.003000 | Total Step Time: 7577.54ms | Tokens/sec: 1383794.28 (accumulated over 5120 batches)\n","Step 2230/10100 | Loss: 0.6106 | LR: 0.003000 | Total Step Time: 7452.14ms | Tokens/sec: 1407079.82 (accumulated over 5120 batches)\n","Step 2231/10100 | Loss: 0.2609 | LR: 0.003000 | Total Step Time: 7636.16ms | Tokens/sec: 1373172.56 (accumulated over 5120 batches)\n","Step 2232/10100 | Loss: 0.3666 | LR: 0.003000 | Total Step Time: 7490.01ms | Tokens/sec: 1399965.42 (accumulated over 5120 batches)\n","Step 2233/10100 | Loss: 0.2641 | LR: 0.003000 | Total Step Time: 7668.32ms | Tokens/sec: 1367413.76 (accumulated over 5120 batches)\n","Step 2234/10100 | Loss: 0.4814 | LR: 0.003000 | Total Step Time: 7486.51ms | Tokens/sec: 1400620.09 (accumulated over 5120 batches)\n","Step 2235/10100 | Loss: 0.4588 | LR: 0.003000 | Total Step Time: 7558.24ms | Tokens/sec: 1387327.84 (accumulated over 5120 batches)\n","Step 2236/10100 | Loss: 0.4536 | LR: 0.003000 | Total Step Time: 7658.60ms | Tokens/sec: 1369149.03 (accumulated over 5120 batches)\n","Step 2237/10100 | Loss: 0.2955 | LR: 0.003000 | Total Step Time: 7657.80ms | Tokens/sec: 1369291.02 (accumulated over 5120 batches)\n","Step 2238/10100 | Loss: 0.3094 | LR: 0.003000 | Total Step Time: 7641.40ms | Tokens/sec: 1372230.20 (accumulated over 5120 batches)\n","Step 2239/10100 | Loss: 0.2841 | LR: 0.003000 | Total Step Time: 7444.65ms | Tokens/sec: 1408495.87 (accumulated over 5120 batches)\n","Step 2240/10100 | Loss: 0.1561 | LR: 0.003000 | Total Step Time: 7748.62ms | Tokens/sec: 1353241.79 (accumulated over 5120 batches)\n","Step 2241/10100 | Loss: 0.2163 | LR: 0.003000 | Total Step Time: 7640.82ms | Tokens/sec: 1372333.91 (accumulated over 5120 batches)\n","Step 2242/10100 | Loss: 0.2966 | LR: 0.003000 | Total Step Time: 7474.35ms | Tokens/sec: 1402898.63 (accumulated over 5120 batches)\n","Step 2243/10100 | Loss: 0.2420 | LR: 0.003000 | Total Step Time: 7646.06ms | Tokens/sec: 1371394.54 (accumulated over 5120 batches)\n","Step 2244/10100 | Loss: 0.9736 | LR: 0.003000 | Total Step Time: 7340.98ms | Tokens/sec: 1428387.63 (accumulated over 5120 batches)\n","Step 2245/10100 | Loss: 0.3337 | LR: 0.003000 | Total Step Time: 7675.15ms | Tokens/sec: 1366195.31 (accumulated over 5120 batches)\n","Step 2246/10100 | Loss: 0.1429 | LR: 0.003000 | Total Step Time: 7570.67ms | Tokens/sec: 1385050.22 (accumulated over 5120 batches)\n","Step 2247/10100 | Loss: 0.3060 | LR: 0.003000 | Total Step Time: 7538.92ms | Tokens/sec: 1390883.90 (accumulated over 5120 batches)\n","Step 2248/10100 | Loss: 0.3015 | LR: 0.003000 | Total Step Time: 7666.33ms | Tokens/sec: 1367767.49 (accumulated over 5120 batches)\n","Step 2249/10100 | Loss: 0.2632 | LR: 0.003000 | Total Step Time: 7480.52ms | Tokens/sec: 1401742.03 (accumulated over 5120 batches)\n","Step 2250/10100 | Loss: 0.6712 | LR: 0.003000 | Total Step Time: 7640.27ms | Tokens/sec: 1372432.87 (accumulated over 5120 batches)\n","\n","==================================================\n","Saving checkpoint at step 2250\n","==================================================\n","Checkpoint saved to: checkpoints/step_2250.pt\n","==================================================\n","\n","Step 2251/10100 | Loss: 0.1518 | LR: 0.003000 | Total Step Time: 7681.63ms | Tokens/sec: 1365044.31 (accumulated over 5120 batches)\n","Step 2252/10100 | Loss: 0.2414 | LR: 0.003000 | Total Step Time: 10666.82ms | Tokens/sec: 983025.67 (accumulated over 5120 batches)\n","Step 2253/10100 | Loss: 0.3412 | LR: 0.003000 | Total Step Time: 8952.29ms | Tokens/sec: 1171293.34 (accumulated over 5120 batches)\n","Step 2254/10100 | Loss: 0.3431 | LR: 0.003000 | Total Step Time: 10699.02ms | Tokens/sec: 980067.71 (accumulated over 5120 batches)\n","Step 2255/10100 | Loss: 0.1903 | LR: 0.003000 | Total Step Time: 10403.87ms | Tokens/sec: 1007870.78 (accumulated over 5120 batches)\n","Step 2256/10100 | Loss: 0.3528 | LR: 0.003000 | Total Step Time: 10315.05ms | Tokens/sec: 1016549.24 (accumulated over 5120 batches)\n","Step 2257/10100 | Loss: 0.2296 | LR: 0.003000 | Total Step Time: 8165.46ms | Tokens/sec: 1284159.59 (accumulated over 5120 batches)\n","Step 2258/10100 | Loss: 0.1174 | LR: 0.003000 | Total Step Time: 7617.90ms | Tokens/sec: 1376463.18 (accumulated over 5120 batches)\n","Step 2259/10100 | Loss: 0.2850 | LR: 0.003000 | Total Step Time: 7455.46ms | Tokens/sec: 1406454.41 (accumulated over 5120 batches)\n","Step 2260/10100 | Loss: 0.1570 | LR: 0.003000 | Total Step Time: 7694.11ms | Tokens/sec: 1362829.55 (accumulated over 5120 batches)\n","Step 2261/10100 | Loss: 0.4196 | LR: 0.003000 | Total Step Time: 7581.84ms | Tokens/sec: 1383009.18 (accumulated over 5120 batches)\n","Step 2262/10100 | Loss: 0.4948 | LR: 0.003000 | Total Step Time: 7497.98ms | Tokens/sec: 1398478.01 (accumulated over 5120 batches)\n","Step 2263/10100 | Loss: 0.5261 | LR: 0.003000 | Total Step Time: 7653.78ms | Tokens/sec: 1370010.47 (accumulated over 5120 batches)\n","Step 2264/10100 | Loss: 0.2568 | LR: 0.003000 | Total Step Time: 7558.34ms | Tokens/sec: 1387309.15 (accumulated over 5120 batches)\n","Step 2265/10100 | Loss: 0.3755 | LR: 0.003000 | Total Step Time: 7792.74ms | Tokens/sec: 1345580.41 (accumulated over 5120 batches)\n","Step 2266/10100 | Loss: 0.1883 | LR: 0.003000 | Total Step Time: 7564.69ms | Tokens/sec: 1386145.26 (accumulated over 5120 batches)\n","Step 2267/10100 | Loss: 0.4405 | LR: 0.003000 | Total Step Time: 7655.27ms | Tokens/sec: 1369744.52 (accumulated over 5120 batches)\n","Step 2268/10100 | Loss: 0.2524 | LR: 0.003000 | Total Step Time: 7699.16ms | Tokens/sec: 1361936.08 (accumulated over 5120 batches)\n","Step 2269/10100 | Loss: 0.1473 | LR: 0.003000 | Total Step Time: 7517.64ms | Tokens/sec: 1394821.31 (accumulated over 5120 batches)\n","Step 2270/10100 | Loss: 0.3349 | LR: 0.003000 | Total Step Time: 7710.95ms | Tokens/sec: 1359852.54 (accumulated over 5120 batches)\n","Step 2271/10100 | Loss: 0.2976 | LR: 0.003000 | Total Step Time: 7472.94ms | Tokens/sec: 1403163.87 (accumulated over 5120 batches)\n","Step 2272/10100 | Loss: 0.4353 | LR: 0.003000 | Total Step Time: 7653.33ms | Tokens/sec: 1370091.90 (accumulated over 5120 batches)\n","Step 2273/10100 | Loss: 0.2039 | LR: 0.003000 | Total Step Time: 7641.06ms | Tokens/sec: 1372290.49 (accumulated over 5120 batches)\n","Step 2274/10100 | Loss: 0.1786 | LR: 0.003000 | Total Step Time: 7553.16ms | Tokens/sec: 1388260.78 (accumulated over 5120 batches)\n","Step 2275/10100 | Loss: 0.2996 | LR: 0.003000 | Total Step Time: 7737.23ms | Tokens/sec: 1355234.11 (accumulated over 5120 batches)\n","Step 2276/10100 | Loss: 0.3072 | LR: 0.003000 | Total Step Time: 7487.51ms | Tokens/sec: 1400434.38 (accumulated over 5120 batches)\n","Step 2277/10100 | Loss: 0.2377 | LR: 0.003000 | Total Step Time: 7719.34ms | Tokens/sec: 1358374.89 (accumulated over 5120 batches)\n","Step 2278/10100 | Loss: 0.8315 | LR: 0.003000 | Total Step Time: 7540.42ms | Tokens/sec: 1390607.76 (accumulated over 5120 batches)\n","Step 2279/10100 | Loss: 0.1843 | LR: 0.003000 | Total Step Time: 7637.52ms | Tokens/sec: 1372927.92 (accumulated over 5120 batches)\n","Step 2280/10100 | Loss: 0.1313 | LR: 0.003000 | Total Step Time: 7752.01ms | Tokens/sec: 1352650.83 (accumulated over 5120 batches)\n","Step 2281/10100 | Loss: 0.2481 | LR: 0.003000 | Total Step Time: 7525.80ms | Tokens/sec: 1393307.96 (accumulated over 5120 batches)\n","Step 2282/10100 | Loss: 0.2341 | LR: 0.003000 | Total Step Time: 7690.08ms | Tokens/sec: 1363544.24 (accumulated over 5120 batches)\n","Step 2283/10100 | Loss: 0.2619 | LR: 0.003000 | Total Step Time: 7521.11ms | Tokens/sec: 1394176.82 (accumulated over 5120 batches)\n","Step 2284/10100 | Loss: 0.1072 | LR: 0.003000 | Total Step Time: 7651.82ms | Tokens/sec: 1370361.83 (accumulated over 5120 batches)\n","Step 2285/10100 | Loss: 0.0959 | LR: 0.003000 | Total Step Time: 7699.32ms | Tokens/sec: 1361908.16 (accumulated over 5120 batches)\n","Step 2286/10100 | Loss: 0.2481 | LR: 0.003000 | Total Step Time: 7467.58ms | Tokens/sec: 1404171.71 (accumulated over 5120 batches)\n","Step 2287/10100 | Loss: 0.2642 | LR: 0.003000 | Total Step Time: 7681.40ms | Tokens/sec: 1365085.37 (accumulated over 5120 batches)\n","Step 2288/10100 | Loss: 0.2168 | LR: 0.003000 | Total Step Time: 7455.52ms | Tokens/sec: 1406442.72 (accumulated over 5120 batches)\n","Step 2289/10100 | Loss: 0.1539 | LR: 0.003000 | Total Step Time: 7710.15ms | Tokens/sec: 1359994.80 (accumulated over 5120 batches)\n","Step 2290/10100 | Loss: 0.1203 | LR: 0.003000 | Total Step Time: 7626.92ms | Tokens/sec: 1374835.50 (accumulated over 5120 batches)\n","Step 2291/10100 | Loss: 0.3284 | LR: 0.003000 | Total Step Time: 7488.53ms | Tokens/sec: 1400242.92 (accumulated over 5120 batches)\n","Step 2292/10100 | Loss: 0.2384 | LR: 0.003000 | Total Step Time: 7693.01ms | Tokens/sec: 1363024.26 (accumulated over 5120 batches)\n","Step 2293/10100 | Loss: 0.3884 | LR: 0.003000 | Total Step Time: 7417.90ms | Tokens/sec: 1413574.71 (accumulated over 5120 batches)\n","Step 2294/10100 | Loss: 0.2380 | LR: 0.003000 | Total Step Time: 7673.71ms | Tokens/sec: 1366452.41 (accumulated over 5120 batches)\n","Step 2295/10100 | Loss: 0.5028 | LR: 0.003000 | Total Step Time: 7440.26ms | Tokens/sec: 1409326.70 (accumulated over 5120 batches)\n","Step 2296/10100 | Loss: 0.2104 | LR: 0.003000 | Total Step Time: 7613.95ms | Tokens/sec: 1377177.98 (accumulated over 5120 batches)\n","Step 2297/10100 | Loss: 0.4453 | LR: 0.003000 | Total Step Time: 7652.05ms | Tokens/sec: 1370319.98 (accumulated over 5120 batches)\n","Step 2298/10100 | Loss: 0.2907 | LR: 0.003000 | Total Step Time: 7446.14ms | Tokens/sec: 1408214.40 (accumulated over 5120 batches)\n","Step 2299/10100 | Loss: 0.4787 | LR: 0.003000 | Total Step Time: 7657.41ms | Tokens/sec: 1369361.41 (accumulated over 5120 batches)\n","Step 2300/10100 | Loss: 0.2627 | LR: 0.003000 | Total Step Time: 7440.79ms | Tokens/sec: 1409226.31 (accumulated over 5120 batches)\n","Step 2301/10100 | Loss: 0.3701 | LR: 0.003000 | Total Step Time: 7630.16ms | Tokens/sec: 1374252.50 (accumulated over 5120 batches)\n","Step 2302/10100 | Loss: 0.1342 | LR: 0.003000 | Total Step Time: 7650.44ms | Tokens/sec: 1370609.31 (accumulated over 5120 batches)\n","Step 2303/10100 | Loss: 0.1589 | LR: 0.003000 | Total Step Time: 7529.71ms | Tokens/sec: 1392585.62 (accumulated over 5120 batches)\n","Step 2304/10100 | Loss: 0.4482 | LR: 0.003000 | Total Step Time: 7602.18ms | Tokens/sec: 1379310.35 (accumulated over 5120 batches)\n","Step 2305/10100 | Loss: 0.3155 | LR: 0.003000 | Total Step Time: 7456.98ms | Tokens/sec: 1406167.11 (accumulated over 5120 batches)\n","Step 2306/10100 | Loss: 0.5056 | LR: 0.003000 | Total Step Time: 7668.00ms | Tokens/sec: 1367470.43 (accumulated over 5120 batches)\n","Step 2307/10100 | Loss: 0.1242 | LR: 0.003000 | Total Step Time: 7528.27ms | Tokens/sec: 1392850.55 (accumulated over 5120 batches)\n","Step 2308/10100 | Loss: 0.6865 | LR: 0.003000 | Total Step Time: 7493.54ms | Tokens/sec: 1399306.28 (accumulated over 5120 batches)\n","Step 2309/10100 | Loss: 0.1075 | LR: 0.003000 | Total Step Time: 7660.63ms | Tokens/sec: 1368785.68 (accumulated over 5120 batches)\n","Step 2310/10100 | Loss: 0.5559 | LR: 0.003000 | Total Step Time: 7385.06ms | Tokens/sec: 1419861.85 (accumulated over 5120 batches)\n","Step 2311/10100 | Loss: 0.3798 | LR: 0.003000 | Total Step Time: 7574.26ms | Tokens/sec: 1384394.64 (accumulated over 5120 batches)\n","Step 2312/10100 | Loss: 0.1018 | LR: 0.003000 | Total Step Time: 7461.45ms | Tokens/sec: 1405325.31 (accumulated over 5120 batches)\n","Step 2313/10100 | Loss: 0.1132 | LR: 0.003000 | Total Step Time: 7698.95ms | Tokens/sec: 1361973.32 (accumulated over 5120 batches)\n"]}],"source":["# Start the training\n","train()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNIu/wOCF7GjDbYkbsoyOS3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"38b4de9a266e418fafae19e0d72b5169":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d5ab2d56f3bb40c2a9b8ea36acb5b143","IPY_MODEL_812e6283e5c2445ebdce2bd7bdb2988b","IPY_MODEL_8c9eac82737d49bf875f5dd5000ac579"],"layout":"IPY_MODEL_e5ac71df2ced49a7987ea643ee5fa5d7"}},"d5ab2d56f3bb40c2a9b8ea36acb5b143":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_580f8106800945c68116ba2c8d4c1b85","placeholder":"","style":"IPY_MODEL_481db2f91e7a4afbb0b4cd5e045595d6","value":"Resolvingdatafiles:100%"}},"812e6283e5c2445ebdce2bd7bdb2988b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c1bcf81a7d949e087fb090f3a926221","max":104,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84ee6bb772844d389f25f0cba18d947d","value":104}},"8c9eac82737d49bf875f5dd5000ac579":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca220176f77f4149b45b911740125b4a","placeholder":"","style":"IPY_MODEL_6af2cb121b0d4498a74891413f33806e","value":"104/104[00:00&lt;00:00,19.15it/s]"}},"e5ac71df2ced49a7987ea643ee5fa5d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"580f8106800945c68116ba2c8d4c1b85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"481db2f91e7a4afbb0b4cd5e045595d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c1bcf81a7d949e087fb090f3a926221":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84ee6bb772844d389f25f0cba18d947d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca220176f77f4149b45b911740125b4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6af2cb121b0d4498a74891413f33806e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a38e03952024b51b589093eb2e9fb39":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc8f7c005c1d452284380d6f3ebe6344","IPY_MODEL_8c161cf383c844f48bb4ce08ed7c31e1","IPY_MODEL_a5922b7c3ab64d82b5a6fe840c6b6090"],"layout":"IPY_MODEL_0720fa09d2ff4362b348f759eeb0627a"}},"dc8f7c005c1d452284380d6f3ebe6344":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0609eb4e4e634ec0b2403775f5316bdc","placeholder":"","style":"IPY_MODEL_b98ee80ecb9f47f48f0fd8368a0f18b4","value":"Resolvingdatafiles:100%"}},"8c161cf383c844f48bb4ce08ed7c31e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_45623ec0539340f39e03c68d81293ca4","max":104,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9fd4c51b397846b79e98b45b2a294462","value":104}},"a5922b7c3ab64d82b5a6fe840c6b6090":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba7049bde9094dddb1f54f418ada3b6d","placeholder":"","style":"IPY_MODEL_91d2a4efc4ef4b63a6b74297427105ae","value":"104/104[00:00&lt;00:00,4978.35it/s]"}},"0720fa09d2ff4362b348f759eeb0627a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0609eb4e4e634ec0b2403775f5316bdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b98ee80ecb9f47f48f0fd8368a0f18b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45623ec0539340f39e03c68d81293ca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fd4c51b397846b79e98b45b2a294462":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba7049bde9094dddb1f54f418ada3b6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91d2a4efc4ef4b63a6b74297427105ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}